[
["index.html", "Introduction to Data Exploration and Analysis with R Welcome to IDEAR", " Introduction to Data Exploration and Analysis with R Michael Mahoney 2018-10-29 Welcome to IDEAR There are only two kinds of programming languages: those people always bitch about and those nobody uses. — Bjarne Stroustrup Figures often beguile me, particularly when I have the arranging of them myself; in which case the remark attributed to Disraeli would often apply with justice and force: ’There are three kinds of lies: lies, damned lies, and statistics. — Mark Twain This is a course reader for a class that will never be taught. Hopefully it helps you nonetheless. Much of this reader draws from Hadley Wickham’s 2012 stat405 class, as well as his wonderful textbooks R for Data Science (with Garrett Grolemund) and Advanced R. Additional steering comes from the YaRrr! textbook. I’ve updated some elements, distilled others, and not kept much of the content, but the ideas and steering come very directly from those works. This course serves as an introduction to R for scientific and research applications, focusing specifically on exploratory data analysis, hypothesis generation and confirmation, data visualization, and communication of results. It is not an introduction to statistics course, though it will teach some statistics. It isn’t even an introduction to computer programming or computer science course, though it will teach some of each of those. This reader is currently being continuously deployed to bookdown.org, particularly as new sections are completed or old ones restructured. This is so that I can get feedback from the small group of people who are using this book to learn R themselves, so I can adjust and adapt the text as needed. If you’d like to help with this process, I’d love to hear from you, at mike.mahoney.218@gmail.com. More information about me can be found at my website, which just so happens to have been built in R. Units 1-5 are currently complete. In production are units 6 and 7 - you can see the skeletons forming at the bottom of this document. There are currently 11 units planned. "],
["introduction-to-r-and-data-visualization.html", "1 Introduction to R and Data Visualization 1.1 Lecture Notes 1.2 Exercises", " 1 Introduction to R and Data Visualization 1.1 Lecture Notes Welcome to the reader for a course that will never be taught! I’ve got a very specific idea of how R should be taught, at least to those interested in using it for data science and other analytical applications. This reader represents that approach - we start off with data visualization, then exploration, and then touch on the basics of the language before getting into data analysis and transformation. But all that comes later. First off, we have to answer one of the most basic questions surrounding this entire book: what even is R? 1.1.1 What is R? R is a programming language used most commonly for data analysis and science. It’s completely free and is adaptable to almost any project - for instance, this book, my website, and plenty of statistical softwares are all written in R. Once you know how to code in R, learning about and implementing those different adaptations is a piece of cake. The purpose of this class is to get you up to speed with the coding! 1.1.2 What is coding? Coding is giving very specific instructions to a very stupid machine. Or rather, a very literal machine - if you do things out of order, or misspell something, or capitalize things you shouldn’t, the computer won’t do it - instead, it’ll give you an error message. But if you get the syntax exactly right, the computer can do very complex tasks very quickly. If that sounds frustrating, well… it sometimes is! But getting good at coding is mostly learning to be okay with being frustrated - and learning what to do when your code is being frustrated. 1.1.3 Conventions of the course (and this reader) We’ll go over standard code styles a bit later in the course - there is a Right Waytm to code, but we won’t worry about that for a few more weeks. But so you can understand a few basics, here’s a few styles we’ll use in the text: If text is preformatted, it means it’s something from R - a function, some data, or anything else that you’d use in your code. Blocks of code will be represented as follows: print(&quot;Hello, World!&quot;) This format both makes it obvious that this is code - not text - and lets you copy and paste it into your R session to see the results for yourself. Code outputs, meanwhile, will mostly be shown like this: ## [1] &quot;Hello, World!&quot; Where text is commented out (that is, has a # in front, so R won’t parse it), so it won’t do anything if you put it in your session. Generally speaking, you should try and type out each block of code in this reader into R. It’s critical that you start getting the muscle memory of typing in R down - that you understand what needs to be capitalized, what needs to be quoted, and what you’re most likely to typo. You can copy and paste straight from this book, but you’ll be setting your own learning back. There are some exceptions to these general rules, but this is enough to get us started. With the introductions out of the way, we can start getting you set up to coding R. 1.1.4 Things You’ll Need There are several pieces of software integral to this course reader, namely: R - download it here RStudio - download it here. Choose the free desktop version - you don’t need the server software, and you don’t need the commercial license. GitHub Desktop - download it here. You don’t need this until later in the course - and if you’re not actually enrolled in the course, you’ll technically never need it. You’ll be better off for knowing it, and it’ll make your life better - but you can get away without it. We’ll be installing other pieces of software (in the form of R packages) throughout this reader, but each of those will be explicitly labeled when we use them. 1.1.5 Introduction to RStudio You’ll almost never need to use R directly in this course - the form of R that you download from that first link is much harder to use than anything professionals interact with. Most practitioners use what’s known as an IDE - an Interactive Development Environment. There’s a lot of choices of IDEs for R, but RStudio is the best one. Other textbooks would give you a more measured, even-handed approach to this issue. RStudio is the best one, though. This course is assuming you’re using RStudio for all the examples, questions, and assignments. As such, we’re going to go over what you’ll see when you open RStudio for the first time. On the bottom right there you’ll see a list of all the files available in the folder that you’re working in. This window will also show you graphs when you make them, and help files when you look for them. Above it is a list of everything that’s available in the current “environment” - that is, all the datasets/functions/code that you’ve already programmed in the session, that can be used again moving forward. On the left - taking up the full height of the pane - is something called the “Console”. This is the first place you can write R code, where you can give very specific commands to that very dumb machine. This is what we’ll be using for this entire unit - whenever you’re typing code from this reader into R, you should be typing it into the console. Try it now - it’s time for us to start coding! 1.1.6 Your First Program Two things to point out, before we get started - if you type either ( or &quot; into RStudio, you’ll notice that the closing mark ) or &quot; are automatically entered for you. If you type the closing mark, it won’t be entered - RStudio is smart enough to prevent you from duplicating this punctuation. However, your programs won’t work if you have any missing punctuation - if you’re getting weird error messages, make sure you have the proper quotes and parentheses! Secondly, if you hit “enter” while working in the console, you’ll tell R to process your code and give you the output. If you want to type more than one line at a time, hold shift while pressing enter. Okay, now let’s get going. Type the following into the console: print(&quot;Hello, world!&quot;) What happened? If you did it right, you should have gotten the following: ## [1] &quot;Hello, world!&quot; It’s cool, right? Congratulations, you’re officially a programmer! What we just did was use a function (print()), R commands that take inputs in the form of arguments (inside the parenthesis) and use them to return an output. We’ll get under the hood of functions in unit 5 - for now, we’re going to use the many functions other people have put together for us. In addition to saying hi, you can use the R console to do math: 2 + 4 / 2 ## [1] 4 6^7 ## [1] 279936 18%%7 ## [1] 4 Note that R generally follows PEMDAS - we’ll go over that later. Also, %% is an operator (in the same way that +, *, -, and / are all operators - mathematical symbols that represent a function) which returns the remainder - the integer (whole number) which remains after you’ve divided as much as you can. So while 7 fits into 18 twice (7*2 = 14), it leaves 4 “left over” afterwards. R can also do a lot of more complicated things. By putting a list of values inside c(), you create what’s known as a vector - a list of objects that R can interact with. c(1, 2, 3) ## [1] 1 2 3 c(18, &quot;ESF&quot;, 98) ## [1] &quot;18&quot; &quot;ESF&quot; &quot;98&quot; c(&quot;ESF&quot;, &quot;Acorns&quot;, &quot;Stumpies&quot;) ## [1] &quot;ESF&quot; &quot;Acorns&quot; &quot;Stumpies&quot; (The “c” stands for combine, by the way.) Look at the difference between that first and that second output - see how the numbers are in quotes the second time around? While R is capable of holding a lot of different types of data, a single vector isn’t. A vector has to either be a numeric or a character vector - it’s either numbers or symbols. This matters, because you can’t do math with symbols. For instance, if we try to divide the first vector by 3: c(1, 2, 3) / 3 ## [1] 0.3333333 0.6666667 1.0000000 It works just fine. Remember that dividing a vector by a scalar (a single number) divides every object in the vector by the scalar. For more information on matrix algebra, click here. Meanwhile, if we tried to divide our second vector: c(18, &quot;ESF&quot;, 98) / 3 # Error in c(18, &quot;ESF&quot;, 98)/3 : non-numeric argument to binary operator We get our first error message of the course. I should mention that I lied a little bit - vectors can also be a third type, logical. If a vector is made up of TRUE and FALSE elements - and no other elements - then the vector is considered logical. Logical values are exactly what they sound like - they return TRUE if something is true, and FALSE if not. For instance: 6 &gt; 4 ## [1] TRUE 4 &gt; 6 ## [1] FALSE Now, the reason I only lied a little is that R understands logical values as binary values - TRUE is 1 and FALSE is 0. For instance: TRUE == 1 ## [1] TRUE FALSE == 0 ## [1] TRUE TRUE + 2 ## [1] 3 18^FALSE ## [1] 1 By the way, see how I used == to prove two things were equal? In R, = does something completely different than == - it assigns a value, which we’ll get into in section 3. For now, just know that you should always use == to check if two values are equivalent. Still, vectors can only hold one type of data - a vector with a value of TRUE and a value of ESF will become a character vector, while c(TRUE, 18, 2) is a numeric vector. If you need to hold more than one type of data, you need a table - or, as they’re called in R, a dataframe. It is possible to make dataframes by hand in R - note, you can press SHIFT+Enter to start a new line: data.frame(x = c(1, 2, 3), y = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), z = c(TRUE, TRUE, FALSE)) ## x y z ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 c FALSE However, this is rare - most of the time, your data will be far too big to make inputting it in R make any sense. Instead, you’ll have to import it from a file elsewhere on your computer - but we’ll get to that later. You’ll see me building basic dataframes using this format throughout this book. I’ll often refer to these as df - the accepted shorthand for dataframe. By the way, if you make a typo, you can press the UP arrow to load the last line of code you sent to R - don’t worry about retyping everything! 1.1.7 The iris Dataset What’s very cool for our purposes is that R comes preloaded with a number of different datasets. Now, if you just type in the name of the dataset, you might overwhelm R for a moment - it will print out every single row of that dataset, no matter how long it is. Luckily for us, the head() command lets us see just the first few rows of the data. If we use the dataset iris (included in base R), for instance, we’d get the following result: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Note that while the default is to print six rows, you can choose how many rows to print by specifying n = ## in the head() function. You can even call the last few rows of your dataset by using the similar function tail() - for instance: tail(iris, n=3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Pretty neat! 1.1.8 Graphing with R This dataset contains measurements on 150 different irises, and is the data that was used to develop the first ever linear regressions. Even if you aren’t a stats nerd, this dataset lets us do some cool things. For instance, we can see that there are columns containing information on sepal length and sepal width - I wonder if they might be correlated? What would happen if we asked R to plot() them for us? Here, we’re going to use the $ operator for the first time - we’ll go over what exactly it does in unit 2, but for now, just know it’s how we specify which columns we want the plot() function to use: plot(iris$Sepal.Length, iris$Sepal.Width) Luckily, as you can see, R has some basic plotting functions built right in. However, these plots are hard to use - and to understand. It seems like sepal length and width are completely unrelated! 1.1.9 The Tidyverse Package Thankfully enough, R has a ton of add-on softwares - called packages - which dramatically expand R’s usefulness. Let’s install some of the most common ones now: install.packages(&quot;tidyverse&quot;) library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.0.0 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Note the quotes around “tidyverse” when you go to install it, but not when it’s inside of library(). The reason for this is a little complicated - basically, you don’t use quotes for things that are inside of R’s memory, like data, functions, and packages. You use quotes for everything else. If you get an error saying “no package named tidyverse”, try reinstalling the package. It might take a few minutes to load. What we just did was install a package called the tidyverse (with install.packages), and load it using library. A “package” is just R code created by someone else - most common problems in R have already been solved by someone else, and most of those people have made their work publicly available for others to use. The tidyverse is a pretty unique example of a package - it actually contains six packages, most of which are essential to using R like a professional. The most important one for us right now is called ggplot2. Don’t worry about having to load it - library(tidyverse) automatically loads this package for you. 1.1.10 ggplot2 ggplot is an attempt to extend R’s basic graphics abilities to make publication-quality graphics faster and easier than ever before. In fact, we can make a version of our scatterplot above, just by typing: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() There are five important steps that went into making that graph: First, the ggplot() call tells R that we want to create a ggplot object Second, the data = iris tells ggplot that everything we do should use the iris dataset Third, the aes() specifies the aesthetics of the graph - what goes on the X and Y axes, but also any other data we want represented in our plot Fourth, the + lets us add additional steps to our plot. Note that the + must always be at the end of a line - putting it at the start of a line will mess up your session! If you see a + in the console instead of a &gt; after trying to plot something, this is most likely what happened - press your escape key to exit the command. Finally, the geom tells ggplot what sort of graph we want. A geom is just the type of plot (or, well, the geometric object which represents data) - so geom_boxplot() generates a boxplot, while geom_col() makes a column chart. geom_point generates a scatterplot, but there are plenty of other options to choose from!x The ggplot() and geom_point calls are known as functions - a type of R object that, when given certain parameters, gives a certain output. Those parameters - in this plot, our data =, x =, and y = calls - are known as arguments. Each of these steps can have different values, if we want to change our graph. For instance, if we wanted to color - and add a trendline for - each species of iris, we could do the following: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; geom_smooth() adds a trendline to your graphs, with a shadow representing the 95% confidence interval around it. While some people refer to this as a line graph, it’s a separate thing entirely - a line graph connects the points, like this: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() + geom_line() For now, we’re going to stick with our pretty smoothed trendline. Our graph makes a lot more sense now - sepal length and width seem to be correlated, but each species is different. If we really wanted to, we could make other aesthetics also change with Species: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point(aes(size = Species, shape = Species)) + geom_smooth(aes(linetype = Species)) ## Warning: Using size for a discrete variable is not advised. ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; But that’s pretty ugly. We’ll get into graph best practices a little bit further into the unit - but generally speaking, a graph should contain exactly as much as it takes to get your point across, and no more. One aesthetic per variable is usually enough. In an important exception to that rule, it’s generally well advised to use different shapes and colors at the same time. Colorblind viewers may not be able to discern the different colors you’re using - so varying the shape of your points or type of your lines helps make your graphics more accessible to the reader. If you want, you can specify shapes using scale_shape functions, such as scale_shape_manual(). There are 25 shapes available for use in ggplot, each of which is named after a number - the number to the left of the shape in the figure below: So if we wanted, we could specify shapes for each species in our dataset pretty easily! I’ve done so below. I’m also going to control the colors by hand - R has a ton of colors available, and you can go crazy picking the best colors for a graph. You can also specify colors by using hex codes (e.g., &quot;#FFFFFF&quot;), but be warned that you might not get an exact match of what you were looking for! ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) + scale_shape_manual(values = c(16, 17, 18)) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) I also made the points a little bigger by specifying size = 3 - note that it isn’t in the aesthetics function, because it doesn’t care about any of the data. We can also vary the type of line that gets drawn when we use geom_smooth. This one only has six options, each of which has both a number and a name: ## Warning: Removed 6 rows containing missing values (geom_path). You can manually specify linetypes with scale_linetype functions, similar to what we did with shapes. You can use either the names or the numbers - just make sure that the names go inside of quotes, while the numbers don’t! I’m going to make our same graph again, manually controlling the linetypes. I’m also going to get rid of that shadow - it represents the 95% confidence interval around the line (which we’ll discuss more in our statistics section), as identified via standard error. We can turn it off by setting se = FALSE in the geom_smooth() function call. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_smooth(aes(linetype = Species), size = 1, se = FALSE) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;twodash&quot;)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We can also combine both graphs into one, more useful graphic: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) + geom_smooth(aes(linetype = Species), size = 1, se = FALSE) + scale_shape_manual(values = c(16, 17, 18)) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;twodash&quot;)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Nifty! Note, by the way, that I’ve put aes() calls in both the ggplot() and geom functions. Geoms inherit from the ggplot() call - they’ll use whatever data and aesthetics are specified inside the parenthesis. However, if you want an aesthetic to only apply to one geom, you can put it inside that geom() call. This is pretty commonly used when an aesthetic only applies to one geom - for instance, our geom_smooth() can’t take a shape =. You have to be careful with this power, though! Sometimes, defining geom-specific aesthetics will give you misleading or simply wrong visualizations. For instance, what would happen if we draw our lines based on the petal length of each species, rather than the sepal width? ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) + geom_smooth(aes(y = Petal.Length, linetype = Species), size = 1, se = FALSE) + scale_shape_manual(values = c(16, 17, 18)) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;twodash&quot;)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Our plot makes no sense! Lots of beginners are tripped up by this when they’re starting - a common assumption is that ggplot will add a second y-axis to the right hand of the plot. In reality, there is no way to graph two y-axes on the same ggplot graph - and that’s on purpose. It’s almost always better to just have two graphs next to each other, if you need to compare the data - though the linked article contains some other interesting suggestions. Anyway, thinking back to our other graphic: This graph is nice, but I think it could be even nicer. Specifically, there’s a lot of overlap between the versicolor and virginica species - it would be nice to see them side by side, rather than on the same plot. 1.1.11 Facetting Luckily, ggplot makes this easy for us via what’s known as facets. By adding facet_wrap() to our plot, we’re able to split the three species onto their own graphs, while keeping the axes standardized. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + facet_wrap(~ Species) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; That makes seeing the differences much easier! Note that I got rid of the different species aesthetics - now that the species are each on their own plot, each species having a different color and shape doesn’t add any information to the visualization. facet_wrap() is very useful, in that it will automatically wrap our plots into however many rows and columns are required. If we want to be a little more specific in how our data is arranged, however, we can use facet_grid(). By specifying either rows = or cols =, we can finely control how our data is split: ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + facet_grid(rows = vars(Species)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Heck, if we have two groups we want to compare, we can use both rows = and cols = at the same time! Unfortunately, iris doesn’t have two grouping variables in it - so I’m going to make another one (color): iris2 &lt;- iris iris2$color &lt;- rep(c(&quot;purple&quot;,&quot;red&quot;,&quot;black&quot;), 50) head(iris2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species color ## 1 5.1 3.5 1.4 0.2 setosa purple ## 2 4.9 3.0 1.4 0.2 setosa red ## 3 4.7 3.2 1.3 0.2 setosa black ## 4 4.6 3.1 1.5 0.2 setosa purple ## 5 5.0 3.6 1.4 0.2 setosa red ## 6 5.4 3.9 1.7 0.4 setosa black As you can see, I’ve told R to replicate the vector of purple, red, black 50 times - so about a third of each species will be in each color. Using that as our column grouping gives us: ggplot(iris2, aes(Sepal.Length, Sepal.Width)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + facet_grid(rows = vars(Species), cols = vars(color)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : pseudoinverse used at 5.2 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : neighborhood radius 0.2 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : reciprocal condition number 0 1.1.12 diamonds For this next exercise, we’re going to be using the diamonds dataset, which contains data about 54,000 different diamond sales. It looks like this: head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.230 Ideal E SI2 61.5 55. 326 3.95 3.98 2.43 ## 2 0.210 Premium E SI1 59.8 61. 326 3.89 3.84 2.31 ## 3 0.230 Good E VS1 56.9 65. 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58. 334 4.20 4.23 2.63 ## 5 0.310 Good J SI2 63.3 58. 335 4.34 4.35 2.75 ## 6 0.240 Very Good J VVS2 62.8 57. 336 3.94 3.96 2.48 We can plot the price of each diamond against its weight (or carat) pretty easily, using geom_point() like before: ggplot(diamonds, aes(carat, price)) + geom_point() Note that I’ve stopped explicitly writing data =, x =, and y =. Without that specification, R assumes that you’re providing arguments to the function in the order the function normally expects them - which, for ggplot(), is in the form ggplot(data, aes(x,y)). Most code you’ll find in the wild is written in this more compact format. Anyway, back to the graph. It’s a bit of a mess! It’s hard to discern a pattern when all 54,000 points are plotted in the same area. We can make things a bit better by making the points transparent, by giving them a low alpha = value: ggplot(diamonds, aes(carat, price)) + geom_point(alpha = 0.05) This is somewhat better! We can see that there’s a correlation between price and carat - but it’s hard to tell exactly what the trend looks like. Plus, there’s a good amount of empty space on the graph, which we could probably make better use of. We can consider transforming our axes to solve all these problems. For instance, if we plotted both our axes on log10 scales, we’d get the following graph: ggplot(diamonds, aes(carat, price)) + geom_point(alpha = 0.05) + scale_x_log10() + scale_y_log10() So we can see that, by log-transforming our variables, we get a linear-looking relationship in our data. Now, I’m personally not a fan of log graphs - and you shouldn’t be, either. But you’ll sometimes have data that can’t be properly explained without logarithims - or advisors who won’t listen to reason. As such, it’s worth knowing how to make R plot things exactly as you want it to. You can perform plenty of other axes transformations by specifying the trans argument inside of your scale function. For instance, if we wanted to use a natural log instead, we could type: ggplot(diamonds, aes(carat, price)) + geom_point(alpha = 0.05) + scale_y_continuous(trans = &quot;log&quot;) + scale_x_continuous(trans = &quot;log&quot;) To learn more about transformations, you can read the documentation by typing ?scale_x_continuous() into the console. 1.1.13 Other Popular Geoms One of the most popular chart types is the simple bar chart. It’s easy enough to make this in ggplot, using geom_bar(): ggplot(diamonds, aes(x = cut)) + geom_bar() Where did count come from? We only specified an x variable! The short answer is that ggplot calculated it by itself! To data scientists, a bar chart is exactly what’s shown here - a graph showing the frequency of each level of a single categorical variable. As such, ggplot only needs an x aesthetic to make a bar plot - it will calculate the count of each level of the variable and use that as its y. If we wanted to communicate more information with this chart, we could think about what number of each cut type is made up of each clarity level. One way to do that is to map the fill of the barplot to the clarity variable: ggplot(diamonds, aes(cut, fill = clarity)) + geom_bar() Note that we use fill in this case, as we’re defining the color for the inside of the polygon, not the lines themselves. If we used color instead, we’d get something like this: ggplot(diamonds, aes(cut, color = clarity)) + geom_bar() Where only the borders of each polygon are colored. Now, ggplot’s default behavior when given a color or fill aesthetic is to make a stacked bar chart, as shown above. Stacked bar charts are awful. It’s really hard to compare values between bars, because the lower limits aren’t standardized. The one exception is if you’re only comparing two values and all bars sum to the same amount, like so: # Make a table of x and y values, which are split into two groups by z. Each x has a y value for each level of z. df &lt;- data.frame(x = c(1, 1, 2, 2, 3, 3), y = c(40, 60, 30, 70, 20, 80), z = c(&quot;A&quot;,&quot;B&quot;,&quot;A&quot;,&quot;B&quot;, &quot;A&quot;, &quot;B&quot;)) df ## x y z ## 1 1 40 A ## 2 1 60 B ## 3 2 30 A ## 4 2 70 B ## 5 3 20 A ## 6 3 80 B ggplot(df, aes(x, y, fill = z)) + geom_col() Note that I’m using geom_col(), which makes column charts. This lets us define y as values other than the simple count - useful if we’re trying to graph the average value for each group, for instance. This simple stacked bar chart works well enough - it lets you compare the values of both A and B, since the groups share a border at either the top or bottom edge of the plot. For most purposes, though, a somewhat better option is the dodged bar chart: ggplot(diamonds, aes(cut, fill = clarity)) + geom_bar(position = &quot;dodge&quot;) Dodged bar plots are better than stacked bars when comparing more than one value for each item on the x axis of a chart. However, with enough series, dodged bar charts can also be decently confusing - try comparing the I1 values between Premium and Fair on this chart, for instance. If you have to have this much information in a single graphic, geom_jitter can help. It generates a scatterplot, much like geom_point(), but “jitters” the points by adding statistical noise - making it easy to compare counts between all combinations of the two variables. ggplot(diamonds, aes(cut, clarity)) + geom_jitter(alpha = 0.05) You can use geom_jitter to make regular scatterplots, as well - for instance, we can see more of the points in our original iris scatterplot by adding a little bit of noise to the plot: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_jitter() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) The last main plot type we’ll go over is the boxplot. This is mostly used to show the distribution of data - it draws a plot with a line at the data’s median, box borders at the 25% and 75% values, and lines reaching to the 5% and 95% values. ggplot(diamonds, aes(cut, price)) + geom_boxplot() There are a lot of other things you can do with ggplot that we won’t go over here - you can find cheatsheets on the package here, and read more documentation here. Note that you can’t make pie charts with ggplot. You usually shouldn’t be using a pie chart anyway, but we’ll go over this in unit 7. 1.1.14 Designing Good Graphics Graphics, at their essence, exist to communicate data and make arguments. In order to do that, a graphic has to be both visually clean and easily understood, while at the same time containing exactly enough information to get a point across - and nothing more. Learning how to make graphics like this is a skill unto itself, and one of the primary focuses of this course. After all, it doesn’t matter how smart you are and how perfect your analysis is if you aren’t able to tell anyone about it afterwards! The hard part about teaching graphic design is that it’s as much an art as a science - there is no one right way to make compelling graphics. What I’m going to teach in this section is as much my opinion as it is the correct way to do things - other textbooks and other people have their own preferred methods, none of which are inherently better or worse. For instance, ggplot comes with a number of preinstalled themes which you can add to any given plot. For a complete list, click here. We’ll just demo a few of the most common ones, using our old iris scatterplot: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + theme_bw() ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + theme_minimal() ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + theme_classic() Plenty of other packages introduce other ggplots for you to use. My personal favorite is cowplot. Written by Claus O. Wilke, it provides some really interesting new extensions to ggplot, and sets the default theme to something that generally looks better than ggplot’s defaults. If we install it now: install.packages(&quot;cowplot&quot;) And then load it: library(cowplot) ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) This default is pretty similar to theme_classic(), except with different font sizes. However, if we add background_grid() to our plot: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() We get what I consider to be the nicest looking default option R will give you. If we want to override the default axis names, we can control that with labs(): ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;) With labs, we can also give our graphs a title and caption. This is generally a bad idea - if you’re going to include a graph in a publication, you’ll want to typeset these outside of the image file - but it makes understanding these graphs a little easier. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;, title = &quot;Sepal Width as a Function of Sepal Length&quot;, subtitle = &quot;Data from R. A. Fischer&#39;s iris dataset, 1936&quot;, caption = &quot;Made in R with ggplot2&quot;) If we want to change anything about the theme (for instance, the text size or legend position), we can specify that in theme(): ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;) + theme(text = element_text(size = 12), axis.text = element_text(size = 10), legend.position = &quot;top&quot;) And we can keep specifying what we want until we’re satisfied with our graph. ggplot will also let us focus on specific parts of the data: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;) + scale_x_continuous(limits = c(5, 7)) + scale_y_continuous(limits = c(2.5, 3.0)) ## Warning: Removed 93 rows containing missing values (geom_point). Of course, if you’re graphing things such as percentages, you should be careful about where you set your axes. Say we had a dataset where every 1 increase in some variable x saw a 1% increase in y, so that y increased almost 10% over the course of all x values. If you let ggplot set your axis defaults, you’d wind up with a perfect correlation: df &lt;- data.frame(x = 1:10, y = 61:70) ggplot(df, aes(x, y)) + geom_line() + background_grid() However, it’s probably more truthful to graph percentages on a 0-100 scale - doing so shows us that x has a weaker impact on y than the default would have us believe: ggplot(df, aes(x, y)) + geom_line() + background_grid() + scale_y_continuous(limits = c(0,100)) 1.1.15 Saving Your Graphics When you’re satisfied with your graph, simply call the ggsave() function to save it to whatever file you’re working in. The first argument to this function should be your graph’s desired file name, with the extension - ggplot can save graphs as pngs, jpegs, pdfs, and several other formats. You can either add it to your workflow with +, or call it after you’ve plotted your graph - ggsave() will save whatever image was drawn last. For more information on specific things ggsave can do, type ?ggsave() into R. 1.1.16 More Resources In addition to the ggplot documentation, I highly reccomend the ggplot book. Additionally, almost any problem can be solved by googling - just put “ggplot” at the end of whatever your question is, and odds are you’ll find the perfect solution. 1.2 Exercises 1.2.1 Calculate the following: The product of 9 * 9 9 squared The remainder from dividing 27 by 2 The remainder of 27 divided by 2, divided by 2 FALSE divided by TRUE. Why did you get this answer? 1.2.2 Graph the following: A boxplot of the iris data, with species on the x axis and sepal length on the y A scatterplot of the iris data, plotting sepal length against width, where the points get bigger as sepal width increases Can you change the color of the boxes in the graph you made for problem 1? Can you change the color of the lines? 1.2.3 Use a new dataset: Also included in ggplot is the mpg dataset, containing fuel economy data for 38 different models of car. Use head() to examine the data. You can also type ?mpg to get documentation on what each variable represents. Is engine displacement (displ) correlated with highway miles per gallon (hwy)? Make a scatterplot to find out. What variables could we use to group the data? Does coloring points by any of these help explain the scatterplot from problem 2? What does the scatterplot look like if you make a scatterplot for cty plotted against hwy? Why? What geom could we use to better represent the data? 1.2.4 Looking ahead: What happens if you type in summary(mpg)? What do you think this output represents? What happens if you type in mpg[1]? How does this compare to mpg[[1]]? "],
["r-functions-and-workflow.html", "2 R Functions and Workflow 2.1 Workflow 2.2 R Functions and Workflow Exercises", " 2 R Functions and Workflow 2.1 Workflow 2.1.1 Scripts So far, we’ve been using the command line interface in the console to type our programs. While this works, you might have noticed how annoying it can be to type longer programs in. Additionally, you’re probably going to want to save your work at some point - and right now, you’d have to use Notepad or a similar program to save anything you’ve done. Luckily, there’s a better way. In the top left corner of RStudio, there’s a menu button called “File”. Click this, then click “New Project”. If you click “New Directory”, and then “New Project”, you’ll be able to create a folder where you can automatically store all of your R code and files. This will also create an R Project file, which you can load to return to where you left off the last time you closed RStudio. Let’s load the tidyverse again, now that we’re in a new directory: library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.0.0 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Now that you’re working in a new directory, go back into “File” and hover over “New File”. There’s a lot of options, but right now we care about two of them: R Scripts and R Notebooks. Open one of each. In your new script file, type the following: ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() Highlight everything and then press Cmd/Ctrl and Enter at the same time. A graph should appear in your Viewer window. Whoops, looks like we forgot to color the points by species - add the color aesthetic to your plot. It should already be clear what the advantage of working with R Scripts is - you can change pieces of your code quickly, without having to worry about retyping things into the console. You can also save and open your scripts (Cmd/Ctrl+S, Cmd/Ctrl+O), which makes working on big projects much easier. Now change your code so it looks like this: a &lt;- ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point(aes(color=Species)) What we’re doing here is assigning the plot to a. Now, anytime you call a, the plot will appear - try it! a Now add the following line under the first two: a + theme(legend.position = &quot;top&quot;) This will move the legend to the top of our graph, much as if we had included the theme() function in our original plot. Your program now has - for the first time in this course - two steps to it: the assignment step (where we make a a plot), and the print step (where we add our theme() and print the plot). While in an R script, there are three ways you can run the whole program: Click the very top of the document and press Cmd/Ctrl+Enter once for each step Highlight the entire code and press Cmd/Ctrl+Enter to run it all at once While your cursor is anywhere in the script, press Cmd/Ctrl+Shift+Enter to run the whole program at once That last method is usually the fastest and easiest. 2.1.2 Notebooks While scripts are great, they do have some drawbacks. For instance, if you have to do more major and slower tasks - like loading datasets and libraries, or complicated math - you’ll have to redo that step every time you want to run the whole program, which is a pain. Also, running a script pulls up the console window, which is a little bit of a headache. For that reason, I tend to work in R Notebooks. Open your new notebook file, and you’ll see a new welcome page! The welcome page has a lot of good information in it - you can delete everything after the second set of three dashes once you’ve read it. Inside a notebook, you can make chunks by pressing Cmd/Ctrl+Alt+I. These chunks run as individual scripts, which you can run the exact same way by using combinations of Cmd/Ctrl, Shift, and Enter. Using notebooks can be a little more efficient than scripts, though, because it offers you the ability to split your code steps into multiple pieces, which can let you iterate on an idea faster than using scripts alone. No matter which you prefer, you should aim to have one script or notebook per task you perform - don’t just have one long, continous notebook for everything you’re doing. Also, make sure you give everything a descriptive name - there’s nothing worse than needing a file a month or so later and having to open every notebook you’ve ever made to find it! It’s also a good idea to make a new R Project, in a new folder, for each major project you start in on. These sorts of things might not matter too much to you while you’re learning - but once you’re doing more complicated things with R, having good habits like these are essential. 2.1.3 Memory, Objects, and Names Let’s go back to when we assigned a plot to a: a &lt;- ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point(aes(color = Species)) The &lt;- symbol is the assignment operator. We can use it to define the object a as all sorts of different objects: # Assign a the value 10 a &lt;- 10 # Print out the object a a ## [1] 10 a &lt;- c(1,50,200) a ## [1] 1 50 200 a &lt;- &quot;Hello, world!&quot; a ## [1] &quot;Hello, world!&quot; a &lt;- geom_point(data = iris, aes(Petal.Length, Petal.Width, color = Species)) ggplot() + a You’ll notice that a is now listed in the upper-lefthand corner of RStudio, under the “Environment” tab. That’s because a is now defined in memory - we can use it in any of our code, anywhere we want. In fact, you can even define a in one file and call it in another, so long as you’ve already run the code defining it in your current RStudio session. This is really cool for a lot of reasons - it lets us do more complicated things with R - but can also cause some problems. If you keep defining objects with names like a, it’s easy to forget which variable stands for what - and so you can wind up making mistakes when using those variables later on. In order to avoid that sort of confusion, you should use descriptive names when creating objects. You should also decide on a standard way you’re going to format those object names - some people prefer snake_case_names, others use.periods, and I personally prefer what’s known as CamelCase. Different organizations and groups have different preferred styles (here’s Google’s), but what’s important right now is that you pick a style that makes sense to you. Be consistent using this style whenever you code - R won’t understand you if you mess up your capitalization! 2.1.4 Dataframes and Transformations Earlier in this course, we went over the different classes of vectors - character, numeric, and logical. If you’re ever trying to find out what class a vector belongs to, you can call the class() function: SampleVector &lt;- c(1,2,3) class(SampleVector) ## [1] &quot;numeric&quot; Note that we don’t put object names (such as the name of our vector) in quotes. The general distinction is that if something exists in the global environment, we don’t put it in quotes. If it isn’t, we do. You can see what’s in the current environment by looking at the “Environment” tab that I mentioned earlier - that tab is a list of all the objects you’ve defined so far in this session. Remember that even though your installed packages aren’t in that list, you still don’t put them in quotes when you call library(). A martix made of vectors is known, in R, as a dataframe. We’ve already seen some simple dataframes in the past unit built using data.frame: data.frame(x = c(1,2,3), y = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), z = c(TRUE, TRUE, FALSE)) ## x y z ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 c FALSE This is an example of something known as rectangular data - the sort you’re likely to find in spreadsheets and many, if not most, scientific applications. We’ll be dealing with rectangular data almost exclusively in this course - while non-rectangular data is useful in many applications, it’s much harder for a beginner to wrap their heads around. In fact, we’ll almost always be working with a very specific type of rectangular data known as tidy data. Tidy dataframes always take the same shape: data.frame(&quot;.&quot; = c(&quot;Observation 1&quot;, &quot;Observation 2&quot;,&quot;...&quot;, &quot;Observation n&quot;), &quot;Variable 1&quot; = c(&quot;Value&quot;, &quot;Value&quot;, &quot;...&quot;, &quot;Value&quot;), &quot;Variable 2&quot; = c(&quot;Value&quot;, &quot;Value&quot;, &quot;...&quot;, &quot;Value&quot;), &quot;Variable ..&quot; = c(&quot;Value&quot;, &quot;Value&quot;, &quot;...&quot;, &quot;Value&quot;), &quot;Variable n&quot; = c(&quot;Value&quot;, &quot;Value&quot;, &quot;...&quot;, &quot;Value&quot;)) ## . Variable.1 Variable.2 Variable... Variable.n ## 1 Observation 1 Value Value Value Value ## 2 Observation 2 Value Value Value Value ## 3 ... ... ... ... ... ## 4 Observation n Value Value Value Value Tidy data is organized as follows: Each column is a single variable Each row is a single observation Each cell is a single value As you might guess from the name, the tidyverse is specifically designed to work with tidy datasets. By storing all data in this format, we’re able to quickly apply the same sets of tools to multiple different types of data. For instance, imagine a dataframe of seasonal temperatures, built as such: SeasonalTemps &lt;- data.frame(Year = c(2015, 2016, 2017, 2018), Winter = c(40, 38, 42, 44), Spring = c(46, 40, 50, 48), Summer = c(70, 62, 81, 76), Fall = c(52, 46, 54, 56)) SeasonalTemps ## Year Winter Spring Summer Fall ## 1 2015 40 46 70 52 ## 2 2016 38 40 62 46 ## 3 2017 42 50 81 54 ## 4 2018 44 48 76 56 This dataframe makes some sense - it’s pretty easy to understand as a human reader, and would probably be a good layout for a printed table. But the problems with this format become obvious when we, for instance, try to graph the data: ggplot(SeasonalTemps, aes(x = Year)) + geom_line(aes(y = Winter), color = &quot;purple&quot;) + geom_line(aes(y = Spring), color = &quot;green&quot;) + geom_line(aes(y = Summer), color = &quot;blue&quot;) + geom_line(aes(y = Fall), color = &quot;red&quot;) What a mess! That took far too long to type - a good general rule of thumb is that if you have to repeat yourself more than twice to do something, there’s a better way to do it. And, even after all our effort, our graph doesn’t have a legend, and the Y axis is labeled wrong. Luckily enough, the tidyverse contains a package designed for making our data tidier - called, helpfully enough, tidyr. We already loaded this package when we called the tidyverse earlier. tidyr provides two essential functions for “reshaping” the data - changing back and forth between the wide format we used above and a long format, easier used by our functions. To change our SeasonalTemps data to a long format, we can use the gather() function. This function gathers values stores in multiple columns into a single variable, and makes another variable - the key variable - representing what column the data was originally in. gather() takes three important arguments: data, the dataframe to gather key, what to name the key column value, what to name the column data was merged into Additionally, we can specify columns that we want to preserve in the new, long dataframe by putting -ColumnName at the end of the function. What this looks like for our seasonal data is something like this: LongTemps &lt;- gather(data = SeasonalTemps, key = Season, value = AvgTemp, -Year) LongTemps ## Year Season AvgTemp ## 1 2015 Winter 40 ## 2 2016 Winter 38 ## 3 2017 Winter 42 ## 4 2018 Winter 44 ## 5 2015 Spring 46 ## 6 2016 Spring 40 ## 7 2017 Spring 50 ## 8 2018 Spring 48 ## 9 2015 Summer 70 ## 10 2016 Summer 62 ## 11 2017 Summer 81 ## 12 2018 Summer 76 ## 13 2015 Fall 52 ## 14 2016 Fall 46 ## 15 2017 Fall 54 ## 16 2018 Fall 56 Note that you don’t have to type data =, key =, and value = - if you don’t, R assumes that you’ve listed the arguments in this order. This format makes graphing significantly easier: ggplot(LongTemps, aes(x = Year, y = AvgTemp, color = Season)) + geom_line() If, after all our hard work, we want to get back to our original wide format, we can undo our gather() using spread(). Again, I’m giving spread a data, key, and value argument - but this time, the function is making a new column for each value of our key: WideTemps &lt;- spread(LongTemps, Season, AvgTemp) WideTemps ## Year Fall Spring Summer Winter ## 1 2015 52 46 70 40 ## 2 2016 46 40 62 38 ## 3 2017 54 50 81 42 ## 4 2018 56 48 76 44 This new dataframe isn’t quite the same as our original - the columns are now in alphabetical order! If we wanted to rearrage them, I find the easiest way is using the select() function from dplyr(), another package in the tidyverse. By giving select() an argument for data and a vector of column names, we can rearrange the order the columns appear: OrderWideTemps &lt;- select(WideTemps, c(Year, Winter, Spring, Summer, Fall)) OrderWideTemps ## Year Winter Spring Summer Fall ## 1 2015 40 46 70 52 ## 2 2016 38 40 62 46 ## 3 2017 42 50 81 54 ## 4 2018 44 48 76 56 When doing this, though, we have to be careful we don’t accidentally forget a column: select(WideTemps, c(Year, Winter, Spring, Fall)) ## Year Winter Spring Fall ## 1 2015 40 46 52 ## 2 2016 38 40 46 ## 3 2017 42 50 54 ## 4 2018 44 48 56 Although, if we wanted to drop a column, we can do so by using a - sign: select(WideTemps, -Summer) ## Year Fall Spring Winter ## 1 2015 52 46 40 ## 2 2016 46 40 38 ## 3 2017 54 50 42 ## 4 2018 56 48 44 2.1.5 The Pipe At this point, we’ve created four dataframes - SeasonalTemps, LongTemps, WideTemps, and OrderedWideTemps - which all contain the same data. When repeatedly making similar but different dataframes, it can be hard to keep track of which object has which data - and it can be hard to keep coming up with simple, descriptive names, too. One solution could be to keep overwriting the same object with the new data: a &lt;- 10 a &lt;- a*2 a &lt;- sqrt(a) But this breaks our rule - that if you have to repeat yourself more than twice, there’s a better way to do it. Plus, if you make a mistake while writing over a value that had your original data in it, you have to start all over again - assuming that your data was saved anywhere else! Luckily, the tidyverse also introduces a new operator %&gt;%, called the pipe. What the pipe does is pretty intuitive - it takes the output of whatever’s on the left side of the pipe, and uses it as the first input to whatever’s on the right side. For instance: Numbers &lt;- c(5,10,15,20,25) Numbers %&gt;% mean() ## [1] 15 Since all of the tidyverse functions take data as their first argument, this lets us chain together multiple functions and skip those assignment steps: LongTemps %&gt;% spread(Season, AvgTemp) %&gt;% select(-Summer) ## Year Fall Spring Winter ## 1 2015 52 46 40 ## 2 2016 46 40 38 ## 3 2017 54 50 42 ## 4 2018 56 48 44 This makes our code much more easy to understand than constantly using the &lt;- operator. While marginally slower than performing repeated assignments, it’s an improved way to perform multiple steps in a way that’s harder to make serious mistakes doing. Even when a function doesn’t have data as its first input, you can still use a pipe by typing data = . into the function: LongTemps %&gt;% spread(data = ., Season, AvgTemp) %&gt;% select(-Summer) ## Year Fall Spring Winter ## 1 2015 52 46 40 ## 2 2016 46 40 38 ## 3 2017 54 50 42 ## 4 2018 56 48 44 And pipes work well with ggplot2, too: LongTemps %&gt;% ggplot(aes(x = Year, y = AvgTemp, color = Season)) + geom_line() 2.1.6 Data Transformations 2.1.6.1 Mutate This becomes useful when we want to transform our data itself for a graph, rather than transform the axes. For example, remember how we made our log-log graph last unit? LongTemps %&gt;% ggplot(aes(x = Year, y = AvgTemp, color = Season)) + geom_line() + scale_y_log10() This is useful, but ggplot only has a certain number of transformations built in (type ?scale_y_continuous() for more info). Additionally, sometimes we’ll want to transform our data for analyses - not just graphing. For this purpose, we can use dplyr’s mutate() function. Mutate takes three arguments: the dataframe (which it can get from %&gt;%), the name of your new column, and what value the new column should have. Say, for example, we wanted to multiply our average temperatures by two: LongTemps %&gt;% mutate(TwiceTemp = AvgTemp * 2) ## Year Season AvgTemp TwiceTemp ## 1 2015 Winter 40 80 ## 2 2016 Winter 38 76 ## 3 2017 Winter 42 84 ## 4 2018 Winter 44 88 ## 5 2015 Spring 46 92 ## 6 2016 Spring 40 80 ## 7 2017 Spring 50 100 ## 8 2018 Spring 48 96 ## 9 2015 Summer 70 140 ## 10 2016 Summer 62 124 ## 11 2017 Summer 81 162 ## 12 2018 Summer 76 152 ## 13 2015 Fall 52 104 ## 14 2016 Fall 46 92 ## 15 2017 Fall 54 108 ## 16 2018 Fall 56 112 You can make multiple columns in the same mutate() call: LongTemps %&gt;% mutate(TwiceTemp = AvgTemp * 2, TwiceSquaredTemp = TwiceTemp^2, YearSeason = paste(Year, Season)) ## Year Season AvgTemp TwiceTemp TwiceSquaredTemp YearSeason ## 1 2015 Winter 40 80 6400 2015 Winter ## 2 2016 Winter 38 76 5776 2016 Winter ## 3 2017 Winter 42 84 7056 2017 Winter ## 4 2018 Winter 44 88 7744 2018 Winter ## 5 2015 Spring 46 92 8464 2015 Spring ## 6 2016 Spring 40 80 6400 2016 Spring ## 7 2017 Spring 50 100 10000 2017 Spring ## 8 2018 Spring 48 96 9216 2018 Spring ## 9 2015 Summer 70 140 19600 2015 Summer ## 10 2016 Summer 62 124 15376 2016 Summer ## 11 2017 Summer 81 162 26244 2017 Summer ## 12 2018 Summer 76 152 23104 2018 Summer ## 13 2015 Fall 52 104 10816 2015 Fall ## 14 2016 Fall 46 92 8464 2016 Fall ## 15 2017 Fall 54 108 11664 2017 Fall ## 16 2018 Fall 56 112 12544 2018 Fall Notice I used a new function, paste(), for that last column. This function pastes together values into a single cell - it can use other values in a dataframe, vectors, or strings. For instance: LongTemps %&gt;% mutate(YearSeason = paste(&quot;The&quot;, Season, &quot;of&quot;, Year)) ## Year Season AvgTemp YearSeason ## 1 2015 Winter 40 The Winter of 2015 ## 2 2016 Winter 38 The Winter of 2016 ## 3 2017 Winter 42 The Winter of 2017 ## 4 2018 Winter 44 The Winter of 2018 ## 5 2015 Spring 46 The Spring of 2015 ## 6 2016 Spring 40 The Spring of 2016 ## 7 2017 Spring 50 The Spring of 2017 ## 8 2018 Spring 48 The Spring of 2018 ## 9 2015 Summer 70 The Summer of 2015 ## 10 2016 Summer 62 The Summer of 2016 ## 11 2017 Summer 81 The Summer of 2017 ## 12 2018 Summer 76 The Summer of 2018 ## 13 2015 Fall 52 The Fall of 2015 ## 14 2016 Fall 46 The Fall of 2016 ## 15 2017 Fall 54 The Fall of 2017 ## 16 2018 Fall 56 The Fall of 2018 Anyway. If you’re transforming your data and don’t want to save the old column, use transmute(): LongTemps %&gt;% mutate(TwiceTemp = AvgTemp * 2) %&gt;% transmute(TwiceSquaredTemp = TwiceTemp^2, YearSeason = paste(Year, Season)) ## TwiceSquaredTemp YearSeason ## 1 6400 2015 Winter ## 2 5776 2016 Winter ## 3 7056 2017 Winter ## 4 7744 2018 Winter ## 5 8464 2015 Spring ## 6 6400 2016 Spring ## 7 10000 2017 Spring ## 8 9216 2018 Spring ## 9 19600 2015 Summer ## 10 15376 2016 Summer ## 11 26244 2017 Summer ## 12 23104 2018 Summer ## 13 10816 2015 Fall ## 14 8464 2016 Fall ## 15 11664 2017 Fall ## 16 12544 2018 Fall 2.1.6.2 Tibbles As I mentioned earlier, data in R is stored in dataframes. However, you may have noticed that the dataframe outputs from tidyverse functions look pretty different in your R session (I’d even say nicer) than our raw datasets! That’s because of another useful tidyverse package, tibble. Of course, the outputs in this book are pretty much the same - the technology I’m using to publish this isn’t quite that advanced, yet. We don’t need to get too far into the mechanics of this package - if you load the tidyverse, any new dataframes you make will be converted into tibbles by default. If you want to force any old dataframe into this format, use as.tibble(); if you need the basic dataframe, use as.data.frame(). 2.1.6.3 Subsetting Data Let’s go back to our iris dataset. I’m going to turn it into a tibble and then view it: iris &lt;- as.tibble(iris) iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa ## 5 5.00 3.60 1.40 0.200 setosa ## 6 5.40 3.90 1.70 0.400 setosa ## 7 4.60 3.40 1.40 0.300 setosa ## 8 5.00 3.40 1.50 0.200 setosa ## 9 4.40 2.90 1.40 0.200 setosa ## 10 4.90 3.10 1.50 0.100 setosa ## # ... with 140 more rows If we only wanted to work with part of this dataset, R gives us a lot of options to subset the data. For instance, if we only wanted the first column containing sepal length, we could type this: iris[, 1] ## # A tibble: 150 x 1 ## Sepal.Length ## &lt;dbl&gt; ## 1 5.10 ## 2 4.90 ## 3 4.70 ## 4 4.60 ## 5 5.00 ## 6 5.40 ## 7 4.60 ## 8 5.00 ## 9 4.40 ## 10 4.90 ## # ... with 140 more rows If we wanted the first row, meanwhile, we’d type this: iris[1, ] ## # A tibble: 1 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa If we wanted several rows, we can specify them with c() or, if they’re consecutive, :. For instance: iris[c(1,2,3,4), ] ## # A tibble: 4 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa iris[1:4, ] ## # A tibble: 4 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa And if we wanted the value in the first row of the first column, we’d type this: iris[1,1] ## # A tibble: 1 x 1 ## Sepal.Length ## &lt;dbl&gt; ## 1 5.10 The pattern should be clear now - inside of the braces, you type the row number, a comma, and then the column number. Notice that [] always gives us a tibble (or dataframe) back. If we wanted a vector, we could use [[]]: iris[[1, 1]] ## [1] 5.1 If we want to use column names instead of numbers, we could use $ in the place of [[]] - note that this always returns a vector, not a dataframe: iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 The $ is really helpful in using other base R functions: mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4403, df = 148, p-value = 0.1519 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269325 0.04351158 ## sample estimates: ## cor ## -0.1175698 (Note that “cor.test()” runs Pearson’s correlation test for whatever vectors you feed it - more on that test later, or here). And $ also lets us filter our data with conditionals - getting values that are equal to something, larger or smaller than it, and so on. For instance, if we want a dataframe (so []) where the rows ([, ]) all have a Species value of (==) “setosa”: iris[iris$Species == &quot;setosa&quot;, ] ## # A tibble: 50 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa ## 5 5.00 3.60 1.40 0.200 setosa ## 6 5.40 3.90 1.70 0.400 setosa ## 7 4.60 3.40 1.40 0.300 setosa ## 8 5.00 3.40 1.50 0.200 setosa ## 9 4.40 2.90 1.40 0.200 setosa ## 10 4.90 3.10 1.50 0.100 setosa ## # ... with 40 more rows Note that the species name is in quotes, because it’s a character string. We don’t have to do that for numeric values: iris[iris$Sepal.Length &gt; 7.5, ] ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7.60 3.00 6.60 2.10 virginica ## 2 7.70 3.80 6.70 2.20 virginica ## 3 7.70 2.60 6.90 2.30 virginica ## 4 7.70 2.80 6.70 2.00 virginica ## 5 7.90 3.80 6.40 2.00 virginica ## 6 7.70 3.00 6.10 2.30 virginica You can use ==, &gt;, &gt;=, &lt;, &lt;=, and != (not equal) to subset your data. 2.1.6.4 Filtering with the Tidyverse This code is hard to read as a human, and doesn’t work well with other functions. For instance, imagine trying to make a scatterplot of just the setosa data - your code will become almost unparseable. Instead, for more involved subsets, dplyr has a useful filter() function. It takes two arguments - your dataframe and the condition it should filter based on: iris %&gt;% filter(Species == &quot;setosa&quot;) ## # A tibble: 50 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa ## 5 5.00 3.60 1.40 0.200 setosa ## 6 5.40 3.90 1.70 0.400 setosa ## 7 4.60 3.40 1.40 0.300 setosa ## 8 5.00 3.40 1.50 0.200 setosa ## 9 4.40 2.90 1.40 0.200 setosa ## 10 4.90 3.10 1.50 0.100 setosa ## # ... with 40 more rows filter() can use all the same operators as the [] methods of subsetting. Additionally, you can use &amp; (“and”) and | (“or”) to chain filters together: iris %&gt;% filter(Species == &quot;setosa&quot; &amp; Sepal.Length == 5.1 &amp; Sepal.Width == 3.3) ## # A tibble: 1 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.30 1.70 0.500 setosa It’s important to remember that &amp; means things which satisfy EACH condition. A common mistake is to type: iris %&gt;% filter(Species == &quot;setosa&quot; &amp; Species == &quot;versicolor&quot;) ## # A tibble: 0 x 5 ## # ... with 5 variables: Sepal.Length &lt;dbl&gt;, Sepal.Width &lt;dbl&gt;, ## # Petal.Length &lt;dbl&gt;, Petal.Width &lt;dbl&gt;, Species &lt;fct&gt; Which, because no flower is both species, returns nothing. In this case, you can either use an | (“or”) operator, or - particularly if you have several cases you want to accept - %in%: iris %&gt;% filter(Species %in% c(&quot;setosa&quot;, &quot;versicolor&quot;)) ## # A tibble: 100 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa ## 5 5.00 3.60 1.40 0.200 setosa ## 6 5.40 3.90 1.70 0.400 setosa ## 7 4.60 3.40 1.40 0.300 setosa ## 8 5.00 3.40 1.50 0.200 setosa ## 9 4.40 2.90 1.40 0.200 setosa ## 10 4.90 3.10 1.50 0.100 setosa ## # ... with 90 more rows So long as your species is %in% the vector c() you provide, it will show up in the output. 2.1.7 Working with Groups Say we wanted to find the mean sepal length in our dataset. That’s pretty easy: mean(iris$Sepal.Length) ## [1] 5.843333 But we already know from our graphs that sepal length differs dramatically between species. If we wanted to find the mean for each species, we could calculate it individually for each group: setosa &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) virginica &lt;- iris %&gt;% filter(Species == &quot;virginica&quot;) versicolor &lt;- iris %&gt;% filter(Species == &quot;versicolor&quot;) mean(setosa$Sepal.Length) ## [1] 5.006 mean(virginica$Sepal.Length) ## [1] 6.588 mean(versicolor$Sepal.Length) ## [1] 5.936 But that code is messy, the output is without any context, and it goes against our rule - that if you have to repeat yourself more than twice, there’s a better way to do it. The better way in the tidyverse is to use grouping and summary functions. In the following example, we’ll use group_by() to group our dataframes by the species types, and summarise() to calculate the mean for each of them (in a column called “MeanSepalLength”): iris %&gt;% group_by(Species) %&gt;% summarise(MeanSepalLength = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species MeanSepalLength ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 This is a faster and easier to understand way to perform functions on groups of data. Note that summarise() uses the British spelling - almost all functions in R have British and American spellings built in (you can use color or colour aesthetics in ggplot, for instance), but this is an important exception. While there is a function called summarize(), it’s highly glitchy and its use is highly discouraged. You can use group_by() to calculate all sorts of things - for instance, we can calculate the distance of each plant’s sepal length from the group mean, as follows: iris %&gt;% group_by(Species) %&gt;% mutate(SLDistanceFromMean = Sepal.Length - mean(Sepal.Length)) ## # A tibble: 150 x 6 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.10 3.50 1.40 0.200 setosa ## 2 4.90 3.00 1.40 0.200 setosa ## 3 4.70 3.20 1.30 0.200 setosa ## 4 4.60 3.10 1.50 0.200 setosa ## 5 5.00 3.60 1.40 0.200 setosa ## 6 5.40 3.90 1.70 0.400 setosa ## 7 4.60 3.40 1.40 0.300 setosa ## 8 5.00 3.40 1.50 0.200 setosa ## 9 4.40 2.90 1.40 0.200 setosa ## 10 4.90 3.10 1.50 0.100 setosa ## # ... with 140 more rows, and 1 more variable: SLDistanceFromMean &lt;dbl&gt; If you want to calculate variables for the whole dataset again, you’ll have to ungroup your data - dataframes will stay grouped until you actively ungroup them with ungroup(). For instance, to calculate the distance of each plant’s sepal length from the overall mean: iris %&gt;% select(c(Sepal.Length, Species)) %&gt;% group_by(Species) %&gt;% mutate(SLDistanceFromGroupMean = Sepal.Length - mean(Sepal.Length)) %&gt;% ungroup() %&gt;% mutate(SLDistanceFromTotalMean = Sepal.Length - mean(Sepal.Length)) ## # A tibble: 150 x 4 ## Sepal.Length Species SLDistanceFromGroupMean SLDistanceFromTotalMean ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.10 setosa 0.0940 -0.743 ## 2 4.90 setosa -0.106 -0.943 ## 3 4.70 setosa -0.306 -1.14 ## 4 4.60 setosa -0.406 -1.24 ## 5 5.00 setosa -0.00600 -0.843 ## 6 5.40 setosa 0.394 -0.443 ## 7 4.60 setosa -0.406 -1.24 ## 8 5.00 setosa -0.00600 -0.843 ## 9 4.40 setosa -0.606 -1.44 ## 10 4.90 setosa -0.106 -0.943 ## # ... with 140 more rows (Note that I got rid of some columns with select() to make all the columns in the tibble fit on one page.) 2.1.8 Missing Values 2.1.8.1 Explicit Missing Values Working with data, there are often two types of missing values we have to worry about. The obvious one are explicit missing values, represented in R as NA (or, sometimes, NaN). Let’s make a dataframe: MissingExample &lt;- tibble(w = c(1, 2, 3), x = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), y = c(&quot;do&quot;, &quot;re&quot;, NA), z = c(807, NA, 780)) MissingExample ## # A tibble: 3 x 4 ## w x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1. A do 807. ## 2 2. B re NA ## 3 3. C &lt;NA&gt; 780. (I’m using tibble() in place of dataframe() here, but the outcome is almost identical.) NA values are a little tricky to work with - look what happens when we try to find the mean of z: mean(MissingExample$z) ## [1] NA The reason this happens is because we don’t know what the mean is - that NA value could be anything, so it’s impossible to know what the mean is. To get around this, we can set the na.rm argument to TRUE: mean(MissingExample$z, na.rm = TRUE) ## [1] 793.5 We can also solve the problem with filtering out the NA values. We can use is.na() to find out where certain values are, and then ask filter() to remove those rows from our dataset as follows: MissingExample %&gt;% filter(!is.na(z)) %&gt;% summarise(Mean = mean(z)) ## # A tibble: 1 x 1 ## Mean ## &lt;dbl&gt; ## 1 794. ! means “negation” in R, or “opposite” - so we’re asking filter() to return the opposite of any row where z is NA, or, alternatively, all the rows where it has a value. If we wanted to drop every row that has a NA, we could use the following tidyr function: MissingExample %&gt;% drop_na() ## # A tibble: 1 x 4 ## w x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1. A do 807. Or, if we knew the values we wanted those NA to represent, we could use replace_na(), also from tidyr. We just have to specify a list of what we want those values to be: MissingExample %&gt;% replace_na(list(y = &quot;mi&quot;, z = &quot;078&quot;)) ## # A tibble: 3 x 4 ## w x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1. A do 807 ## 2 2. B re 078 ## 3 3. C mi 780 Notice a difference in the z column with this example? Because I put “078” in quotes, it changed the entire column to a character vector - because quotes mean characters, and a vector can only hold one class of data. We’ll talk more about that list() function later on - that’s a little too complicated for this unit. 2.1.8.2 Implicit Missing Values The other, harder to identify type of missing value is the implicit missing value. Say we have a dataframe TreeData, which lists the species that are present at two different sites: TreeData &lt;- tibble(Site = c(&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;), Species = c(&quot;Red Maple&quot;, &quot;Sugar Maple&quot;, &quot;Black Cherry&quot;, &quot;Red Maple&quot;, &quot;Sugar Maple&quot;), Count = c(10,5,15,8,19)) TreeData ## # A tibble: 5 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A Red Maple 10. ## 2 A Sugar Maple 5. ## 3 A Black Cherry 15. ## 4 B Red Maple 8. ## 5 B Sugar Maple 19. A lot of field data is collected like this, where each row represents something that was present at the field site. The problem with this comes when we try to calculate summary statistics for each species: TreeData %&gt;% group_by(Species) %&gt;% summarise(Mean = mean(Count), StandardDev = sd(Count)) ## # A tibble: 3 x 3 ## Species Mean StandardDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Black Cherry 15. NaN ## 2 Red Maple 9. 1.41 ## 3 Sugar Maple 12. 9.90 Black cherry has a missing (NaN) standard deviation, because as far as R knows, it only has one observation to make estimates with. In reality, the fact that black cherry was missing from site B is a data point in and of itself - it’s an implicit value of 0. To fix that, we can use the complete() command from tidyr. This function takes column names as arguments, and returns a dataframe with every combination of the values in those columns. We can also specify what to replace NA values with, much like we did in replace_na(), with fill: TreeData %&gt;% complete(Site, Species, fill = list(Count = 0)) ## # A tibble: 6 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A Black Cherry 15. ## 2 A Red Maple 10. ## 3 A Sugar Maple 5. ## 4 B Black Cherry 0. ## 5 B Red Maple 8. ## 6 B Sugar Maple 19. This way, when we go to calculate our summary statistics, we get better answers: TreeData %&gt;% complete(Site, Species, fill = list(Count = 0)) %&gt;% group_by(Species) %&gt;% summarise(Mean = mean(Count), StandardDev = sd(Count)) ## # A tibble: 3 x 3 ## Species Mean StandardDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Black Cherry 7.50 10.6 ## 2 Red Maple 9.00 1.41 ## 3 Sugar Maple 12.0 9.90 2.1.9 Count Data One other common issue with field data is that it’s in a summary form - for instance, our tree data summarizes the number of trees at each site into one column. This is often easier to record in the field and easier to read as a human - but it makes some analyses much harder! The function uncount() makes this pretty easy for us: LongTreeData &lt;- TreeData %&gt;% uncount(Count) LongTreeData ## # A tibble: 57 x 2 ## Site Species ## &lt;chr&gt; &lt;chr&gt; ## 1 A Red Maple ## 2 A Red Maple ## 3 A Red Maple ## 4 A Red Maple ## 5 A Red Maple ## 6 A Red Maple ## 7 A Red Maple ## 8 A Red Maple ## 9 A Red Maple ## 10 A Red Maple ## # ... with 47 more rows And if we wanted to get back to the summary table, we can use count(): LongTreeData %&gt;% count(Site, Species) ## # A tibble: 5 x 3 ## Site Species n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 A Black Cherry 15 ## 2 A Red Maple 10 ## 3 A Sugar Maple 5 ## 4 B Red Maple 8 ## 5 B Sugar Maple 19 If we want to change that column n’s name to something more descriptive, we can use rename(): LongTreeData %&gt;% count(Site, Species) %&gt;% rename(Count = n) ## # A tibble: 5 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 A Black Cherry 15 ## 2 A Red Maple 10 ## 3 A Sugar Maple 5 ## 4 B Red Maple 8 ## 5 B Sugar Maple 19 2.1.10 Oddballs Predict what will happen when you run the following code - then run it! sqrt(2)^2 == 2 1/49 * 49 == 1 You’d expect both of these things to be true, but R seems to think otherwise. That’s because R has to estimate the true value of things like 1/49 - it only calculates to so many digits, because it can’t store an infinite number of decimal places. As such, 1/49 * 49 isn’t exactly equal to 1 - it’s just near it. To catch these sorts of things, use near() instead of ==: 1/49 * 49 == 1 ## [1] FALSE near(1/49 * 49, 1) ## [1] TRUE 2.2 R Functions and Workflow Exercises 2.2.1 Do the following: What class is the vector c(1, TRUE, 3)? Why is it not a character vector? Make and print this tibble. What do the abbreviations under each column name mean? tibble(x = c(1, 2, 3), y = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), z = c(TRUE, FALSE, TRUE)) Inspect the smiths dataset (loaded with the tidyverse - you can access it like iris). How can you fix those missing values? 2.2.2 Work with other datasets: spread() the iris dataset so that each species’ petal width is in its own column. Then gather() the table back together. What’s different about this dataframe? Select all the rows of iris where the species is setosa. Now select all the rows where the species isn’t setosa. What’s the mean price for each cut of diamond in the diamonds dataset? "],
["basic-statistics-using-r.html", "3 Basic Statistics (Using R) 3.1 Purpose of the Unit 3.2 Definitions 3.3 Examples", " 3 Basic Statistics (Using R) 3.1 Purpose of the Unit This isn’t a statistics course. However, R is a statistical computing language, and many of the functions built into R are designed for statistical purposes. As such, we’re going to very quickly go over some statistical terms and a few of the statistical functions built into R. That way, you’ll have a better understanding of what exactly it is that we’re doing later in the course. There aren’t any exercises with this unit, since we haven’t worked through using these functions with examples yet. However, we’ll be using most of these functions - in combination with those from Unit 2 - for the rest of this course. 3.2 Definitions 3.2.1 Data Concepts We’ve already discussed some data concepts in this course, such as the ideas of rectangular and tidy data. However, those discussions are buried in the text of the last unit, so are hard to refer to - and I want to make sure these concepts are discussed explicitly, so I can refer to them in the future without explaining them. Vector Sequence of data elements of the same type Each element of the vector are also called components, members, or values Created in R using c() Dataframe A list of vectors of identical lengths Example: iris Variable A trait or condition that can exist in different quantities or types We measure the impacts of independent predictor variables on dependent response variables Continuous Data Numeric data which is not restricted to certain values - there are an infinite number of possible values Discrete Data Numeric data which is restricted to certain values - for example, number of kids (or trees, or animals) Categorical Data Data which can only exist as one of a specific set of values - for example, house color or zip code Binned numeric data (e.g. “between 1 and 2 inches”) is typically categorical Binary Data Categorical data where the only values are 0 and 1 Often used in situations where a “hit” - an animal getting trapped, a tree falling down, etc - is a 1, and no hit is a 0 Ordinal Data A type of categorical data where each value is assigned a level or rank Useful with binned data, but also in graphing to rearrange the order categories are drawn Referred to in R as “factors” Data Distribution How often every possible value occurs in a dataset Usually shown as a curved line on a graph, or a histogram Normal Distribution Data where mean = median, 2/3 of the data are within one standard deviation of the mean, 95% of the data are within two SD and 97% are within 3. Many statistical analyses assume your data are normally distributed Many datasets - especially in nature - aren’t Skewed Distribution Data where the median does not equal the mean A left-skewed distribution has a long tail on the left side of the graph, while a right-skewed distribution has a long tail to the right Named after the tail and not the peak of the graph, as values in that tail occur more often than would be expected with a normal distribution 3.2.2 Statistical Terms Estimate A statistic calculated from your data Called an estimate as we are approximating population-level values from sample data Synonynm: metric Hypothesis Testing Comparing the null hypothesis (typically, that two quantities are equivalent) to an alternative hypothesis The alternative hypothesis in a two-tailed test is that the quantities are different, while the alternative hypothesis in a one-tailed test is that one quantity is larger or smaller than the other p Value: The probability of seeing an effect of the same size as our results given a random model High p values often mean your independent variables are irrelevant, but low p values don’t mean they’re important - that judgement requires scientific justification, and examining the effect size and importance. Otherwise you’re just equating correlation and causation. The 0.05 thing is from a single sentence, taken out of context, from a book published in 1925. There’s no reason to set a line in the sand for “significance” - 0.05 means that there’s a 1 in 20 prpbability your result could be random chance, and 0.056 means it’s 1 in 18. Those are identical odds. Some journals have banned their use altogether, but others still will only accept “significant” results Statement from the American Statistical Association: A p value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p value does not provide a good measure of evidence about a model or a hypothesis. “Robust” A term meaning an estimate is less susceptible to outliers Means are not robust, while medians are, for instance. Regression A method to analyze the impacts of independent variables on a dependent variable ANOVA and models are both types of regression analyses General Linear Model Formulas representing the expected value of a response variable for given values of one or more predictors Sometimes abbreviated GLM Generalized Linear Model Depending who you ask, these may or may not be linear models In this course, we’ll only be using logistic models Sometimes abbreviated GLM 3.2.2.1 Estimates and Statistics n The number of observations of a dataset or level of a categorical. In R, run nrow(dataframe) or length(Vector) to calculate. To calculate by group, run count(Data, GroupingVariable) Examples: nrow(iris), length(iris$Sepal.Length), count(iris, Species) Mean The average of a dataset, defined as the sum of all observations divided by the number of observations. In R, run mean(Vector) to calculate. Example: mean(iris$Sepal.Length) Trimmed Mean The mean of a dataset with a certain proportion of data not included The highest and lowest values are trimmed - for instance, the 10% trimmed mean will use the middle 80% of your data mean(Vector, trim = 0.##) mean(iris$Sepal.Length, trim = 0.10) Variance A measure of the spread of your data. var(Vector) var(iris$Sepal.Length) Standard Deviation The amount any observation can be expected to differ from the mean. sd(Vector) sd(iris$Sepal.Length) Standard Error The error associated with a point estimate (e.g. the mean) of the sample. If you’re reporting the mean of a sample variable, use the SD. If you’re putting error bars around means on a graph, use the SE. No native function in R - run sd(Vector)/sqrt(length(Vector)) to calculate. sd(iris$Sepal.Length)/sqrt(length(iris$Sepal.Length)) Median Absolute Deviation from the Median Average distance between each datapoint and - in R - the median A measure of spread in your data Note that MAD often means mean absolute deviation from the mean, mean absolute deviation from the median, and a few other less common things - check your acronyms before using! mad(Vector) mad(iris$Sepal.Length) Median A robust estimate of the center of the data. median(Vector) median(iris$Sepal.Length) Minimum The smallest value. min(Vector) min(iris$Sepal.Length) Maximum The largest value. max(Vector) max(iris$Sepal.Length) Range The maximum minus the minimum. max(Vector) - min(Vector) max(iris$Sepal.Length) - min(iris$Sepal.Length) Quantile The n quantile is the value at which, for a given vector, n percent of the data is below that value. Ranges from 0-1. Quantile * 100 = percentile. Quartiles are the 0.25, 0.5, and 0.75 quantiles quantile(Vector, c(quantiles that you want)) quantile(iris$Sepal.Length, c(0.25, 0.5, 0.75)) Interquartile Range The middle 50% of the data, contained between the 0.25 and 0.75 quantiles IQR(Vector) IQR(iris$Sepal.Length) Skew The relative position of the mean and median. At 0, mean = median, and the data is normally distributed. Not included in base R. Kurtosis The size of the tails in a distribution. In R, values much different from 0 are non-normally distributed. Not included in base R. 3.2.3 Models and Tests Note that most tests discussed here default to a 95% confidence level and a two-tailed test. If you want to learn how to change those for any function, type ?FunctionName(). Functions not applicable to the iris() dataset do not currently have examples underneath them. Correlation How closely related two variables are Pearson’s test assumes your data is normally distributed and measures linear correlation Spearman’s test does not assume normality and measures non-linear correlation Kendall’s test also does not assume normality and measures non-linear correlation, and is a more robust test - but it is harder to compute by hand, and as such is less commonly seen You cannot compare results from one type of test to another - Kendall’s results are always 20-40% lower than Spearman’s, for instance cor(Vector1, Vector2) provides correlation coefficients, while cor.test(Vector1, Vector2) performs the statistical test, giving test statistics, p values, and other outputs. Both perform the Pearson test by default, but can be changed by providing the argument method = &quot;spearman&quot; or method = &quot;kendall&quot; cor(iris$Sepal.Length, iris$Sepal.Width, method = &quot;pearson&quot;) cor.test(iris$Sepal.Length, iris$Sepal.Width, method = &quot;pearson&quot;) t Test A method of comparing the means of two groups If your group variable has more than two levels, don’t use a t test - use an ANOVA instead t.test(Vector1, Vector2) Chi-squared Test A test to see if two categorical variables are related The null hypothesis is that both variables are independent from one another For more information - particularly on what form your data should be in - please see this site. chisq.test(Vector1, Vector2) Linear Models A type of regression which predicts the value of a response variable at given values of independent predictor variables If you need help understanding linear models, here’s a lecture from Penn State. The most important thing for our purposes is understanding that models are made up of terms, which are the predictor variables you give it, and coefficients, which are multiplied by those terms to calculate the value of our response. Larger coefficients generally mean that variable is more important than others in the model, but only if the variables share units - one (much argued-over) way to compare coefficients between variables with different units is discussed here Linear models also include an intercept term, which isn’t multiplied with any variable. This term being “significant” at p = whatever means literally nothing, and if it’s your only significant term, your model is useless. You can add multiple predictor vectors using +, and include interaction terms using + Vector1:Vector2. lm(ResponseVector ~ PredictorVectors, data) lm(Sepal.Length ~ Species, data = iris) Logistic Models A form of generalized linear model where the predictor variable is a binary vector Extremely common in science and business to predict events - if a tree or animal will die, if a sale will be made, etc glm(ResponseVector ~ PredictorVectors, data, family = &quot;binomial&quot;) ANOVA A test to identify the impacts of one or more categorical variables on one or more numeric response variables An altered form of the linear model This blog post does an interesting treatment on how to interpret ANOVA results with significant interactions anova(lm(ResponseVector ~ PredictorVectors, data)) Normally, you’ll save your model to an object using &lt;-, then run anova() on that object - the way I wrote it above is just demonstrative model &lt;- lm(Sepal.Length ~ Species, data = iris), then anova(model) 3.2.4 How We’ll Compare Models There are plenty of different ways to compare models, each with their own proponents and detractors. Rather than wade into those arguments, we’re going to use three of the most common metrics. Other metrics (RMSE, PRESS, BIC, etc) can also be generated from R’s modeling functions, but we won’t go into depth on those. R2 The percentage of variance in your data explained by your regression The more independent predictor variables in a model, the higher the R2, all else being equal The adjusted R2 is a better estimate of regression goodness-of-fit, as it adjusts for the number of variables in a model summary(lm(ResponseVector ~ PredictorVector, data)) model &lt;- lm(Sepal.Length ~ Species, iris) then summary(model) Receiver Operating Characteristic Curve R2 isn’t applicable to logistic models Instead, we calculate the area under the ROC curve, with the area under the curve being abbreviated AUC AUC represents the accuracy of your model, with random guessing having an AUC of 0.5 and a perfect model having an AUC of 1 AUC is sometimes referred to as the c-statistic AIC: Akaike Information Criterion For a single dataset, the model with the smallest AIC is your best model But models with a \\(\\Delta\\)AIC (the difference between their two AICs) of &lt; 2 (or 4, depending who you ask) are statistically identical If your model has a \\(\\Delta\\)AIC of &lt; 2 with the null model, it’s useless Null model: lm(ResponseVariable ~ 1) 3.3 Examples All examples here use the Orange dataset, which is automatically included in R. Note that the O is capitalized! Look at Orange using either head or as.tibble() (you’ll have to run library(tidyverse) for that second option). What type of data are each of the columns? Find the mean, standard deviation, and standard error of tree circumference. Make a linear model which describes circumference (the response) as a function of age (the predictor). Save it as an object with &lt;-, then print the object out by typing its name. What do those coefficients mean? Make another linear model describing age as a function of circumference. Save this as a different object. Call summary() on both of your model objects. What do you notice? Does this mean that trees growing makes them get older? Does a tree getting older make it grow larger? Or are these just correlations? Does the significant p value prove that trees growing makes them get older? Why not? "],
["introductory-data-analysis.html", "4 Introductory Data Analysis 4.1 Exploratory Data Analysis 4.2 Statistical Tests and Regressions 4.3 Functional Programming 4.4 Modeling 4.5 Mixed Models 4.6 Conclusion 4.7 Exercises", " 4 Introductory Data Analysis 4.1 Exploratory Data Analysis So far, we’ve learned about how to manipulate our data and how to graph our outputs. Both of these are critically important parts of what’s known as exploratory data analysis - or EDA. When you’re starting with a new dataset, you won’t always immediately know what trends and patterns might be there to discover. The idea at this stage isn’t to find out what’s causing any trends in the data, to identify any significant results you might have, or to get publishable figures and tables - the point is to understand exactly what it is that you’re dealing with. This unit gives examples of what EDA might look like with a sample dataset. But there aren’t prescribed sets of steps to go through while working on EDA - you should feel free to create as many hypotheses as possible, and spend time analyzing them individually. You might find something surprising! Speaking of surprises, I really enjoy this quote from Nate Silver, founder and editor in chief of FiveThirtyEight: You ideally want to find yourself surprised by the data some of the time — just not too often. If you never come up with a result that surprises you, it generally means that you didn’t spend a lot of time actually looking at the data; instead, you just imparted your assumptions onto your analysis and engaged in a fancy form of confirmation bias. If you’re constantly surprised, on the other hand, more often than not that means your [code] is buggy or you don’t know the field well enough; a lot of the “surprises” are really just mistakes. —Nate Silver Surprises are awesome, and are how discoveries are made in science. But at the same time, a lot of papers are retracted because their big surprise was actually just a glitch in the code. Whenever you find something you didn’t expect, make sure you go back through your code and assumptions - it never hurts to double check! 4.1.1 gapminder Anyway. We’ll be working with data from the gapminder database, which contains statistics on global development metrics. We can get the data like we get most packages: install.packages(&quot;gapminder&quot;) library(gapminder) Let’s also load the tidyverse: library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.0.0 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() The gapminder package includes four tables, of which we only care about one: gapminder. We can preview the data, as usual, by typing the name of the table: gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ... with 1,694 more rows 4.1.2 Describing Your Data If we want to get a sense of each variable, we can use base R’s summary() to find basic summary statistics for each column: summary(gapminder) ## country continent year lifeExp ## Afghanistan: 12 Africa :624 Min. :1952 Min. :23.60 ## Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.20 ## Algeria : 12 Asia :396 Median :1980 Median :60.71 ## Angola : 12 Europe :360 Mean :1980 Mean :59.47 ## Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.85 ## Australia : 12 Max. :2007 Max. :82.60 ## (Other) :1632 ## pop gdpPercap ## Min. :6.001e+04 Min. : 241.2 ## 1st Qu.:2.794e+06 1st Qu.: 1202.1 ## Median :7.024e+06 Median : 3531.8 ## Mean :2.960e+07 Mean : 7215.3 ## 3rd Qu.:1.959e+07 3rd Qu.: 9325.5 ## Max. :1.319e+09 Max. :113523.1 ## For the categorical variables country and continent, R just returns the number of observations each category has. For the numeric variables, meanwhile, the output is a little more involved. We can get an even fuller sense of the data using functions from other packages. My personal favorite is the describe() function included in psych: install.packages(&quot;psych&quot;) psych::describe(gapminder) ## vars n mean sd median trimmed ## country* 1 1704 71.50 41.00 71.50 71.50 ## continent* 2 1704 2.33 1.21 2.00 2.27 ## year 3 1704 1979.50 17.27 1979.50 1979.50 ## lifeExp 4 1704 59.47 12.92 60.71 59.92 ## pop 5 1704 29601212.32 106157896.74 7023595.50 11399459.45 ## gdpPercap 6 1704 7215.33 9857.45 3531.85 5221.44 ## mad min max range skew kurtosis ## country* 52.63 1.00 142.0 141 0.00 -1.20 ## continent* 1.48 1.00 5.0 4 0.25 -1.34 ## year 22.24 1952.00 2007.0 55 0.00 -1.22 ## lifeExp 16.10 23.60 82.6 59 -0.25 -1.13 ## pop 7841473.62 60011.00 1318683096.0 1318623085 8.33 77.62 ## gdpPercap 4007.61 241.17 113523.1 113282 3.84 27.40 ## se ## country* 0.99 ## continent* 0.03 ## year 0.42 ## lifeExp 0.31 ## pop 2571683.45 ## gdpPercap 238.80 (Note: psych:: lets me call functions from the psych package without having to load it using library(). This is useful when you aren’t using the functions that often - and lets me explicitly identify which functions come from which packages, for the purposes of instruction - but becomes less useful when using the same function or package multiple times. Imagine having to type dplyr:: everytime we wanted to use %&gt;%, for instance!) This output replaces the IQR with the full range of the data, and adds a number of other important statistics to the output. For instance, we can now see the skew and kurtosis of our data, representing how close our data is to the normal distribution. Briefly, skew represents how close the median is to the mean (or how long the data’s “tails” are - 0 means the median is the mean), while kurtosis represents how large the tails of the distribution are (with normally distributed data having a kurtosis of 0). You should always analyze these values for your data - both in order to give you a sense of what you’re working with, and to tell you if there were any errors during data entry. If population had a negative minimum value, for instance, you’d know to be alarmed. As we can see, some of our data is highly skewed, with extremely long tails. For instance, if we were to make a histogram of population: ggplot(gapminder, aes(pop)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This makes some intuitive sense - most countries have decently small populations, while some countries - such as China and India - contain significant portions of the world’s population. Now that we have a sense of what our data looks like, we can start attempting to identify trends in the data. You should never trust your data before visualizing it - summary statistics and other tests may not give you critical insights about trends present in your data. Base R’s pairs() function is useful for this purpose - it makes a matrix of scatterplots for all your variables, letting you see any correlations that exist visually. Note that we have to subset our data to make sure that we’re only graphing numeric columns: pairs(gapminder[, 3:6]) From this, we can immediately see some trends - it looks like all our variables are increasing over time (left column), and that life expectancy goes up as GDP increases (right column, second from the top). A way to see the same thing in table form is to use cor(): cor(gapminder[, 3:6]) ## year lifeExp pop gdpPercap ## year 1.00000000 0.43561122 0.08230808 0.22731807 ## lifeExp 0.43561122 1.00000000 0.06495537 0.58370622 ## pop 0.08230808 0.06495537 1.00000000 -0.02559958 ## gdpPercap 0.22731807 0.58370622 -0.02559958 1.00000000 The numbers refer to how correlated the variables are: 1 means perfectly positively correlated, while -1 is perfectly negatively correlated. If we wanted to get even fancier, we could coerce this result to a dataframe using the function tidy() from the package broom. We can even visualize this data in ggplot, using geom_tile(): install.packages(&quot;broom&quot;) CorMatrix &lt;- broom::tidy(cor(gapminder[, 3:6])) %&gt;% rename(Var1 = &quot;.rownames&quot;) %&gt;% gather(Var2, Cor, -Var1) CorMatrix ## Var1 Var2 Cor ## 1 year year 1.00000000 ## 2 lifeExp year 0.43561122 ## 3 pop year 0.08230808 ## 4 gdpPercap year 0.22731807 ## 5 year lifeExp 0.43561122 ## 6 lifeExp lifeExp 1.00000000 ## 7 pop lifeExp 0.06495537 ## 8 gdpPercap lifeExp 0.58370622 ## 9 year pop 0.08230808 ## 10 lifeExp pop 0.06495537 ## 11 pop pop 1.00000000 ## 12 gdpPercap pop -0.02559958 ## 13 year gdpPercap 0.22731807 ## 14 lifeExp gdpPercap 0.58370622 ## 15 pop gdpPercap -0.02559958 ## 16 gdpPercap gdpPercap 1.00000000 ggplot(CorMatrix, aes(Var1, Var2, fill = Cor)) + geom_tile() Of course, there’s no need to do all of these at once - you can do whichever method makes sense to you. Either way, we get similar results - the strongest correlation is between life expectancy and GDP. We can visualize this trend using our ggplot skills: ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_jitter() Hmm, weird! While we can see the correlation, it seems like there’s another factor we aren’t accounting for. What happens if we color the points by the year they represent? ggplot(gapminder, aes(gdpPercap, lifeExp, color = year)) + geom_jitter() That looks like we’re onto something. What if we facetted the data by continent? ggplot(gapminder, aes(gdpPercap, lifeExp, color = year)) + geom_jitter() + facet_wrap(~ continent) Hmm. We can see some obvious trends - it seems like Africa has a lower average life expectancy than the other continents, for instance - but they’re hard to discern from the visuals alone. For that, we’re going to have to get into some actual statistic computing. 4.2 Statistical Tests and Regressions Now that we’ve identified some hypotheses about our data, we’re able to use the full power of R to try and prove them. First off, we can identify if any of the correlations we saw are statistically significant. For the full dataset, this is pretty easy - we can just feed two vectors into cor.test(): cor.test(gapminder$lifeExp, gapminder$gdpPercap) ## ## Pearson&#39;s product-moment correlation ## ## data: gapminder$lifeExp and gapminder$gdpPercap ## t = 29.658, df = 1702, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5515065 0.6141690 ## sample estimates: ## cor ## 0.5837062 (Note that most likely we’d actually use a different correlation test, as the data on life expectancy are non-normal. You can control the test used by setting the method argument - information about the correlation tests available in this function can be found [here] (http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/).) Looks like a yes! Before we rush off to publish, though, it might worth seeing if the correlations are different (or, hopefully, stronger) on each continent. Before we do that, let’s see if the different continents really do have different life expectancies. In order to do this, we’ll perform an ANOVA using our continents as different groups, then test for differences between the groups. Now, ANOVA is actually just another form of a linear regression, where you’re predicting the impact an independent variable has on the response variable. The unique thing about ANOVA is that the independent variables have to be categorical - if they’re all numeric, you want to stick with regression, and if there’s a mix, you’ll use ANCOVA. The reason I bring this up is that running an ANOVA in R requires you to first build a linear model, using the aov() function. This function is pretty simple, taking three arguments - the response variable, all independent variables, and the data to be used. For our purposes, modeling life expectancy as a function of continent, the equation looks like this: aov(lifeExp ~ continent, data = gapminder) ## Call: ## aov(formula = lifeExp ~ continent, data = gapminder) ## ## Terms: ## continent Residuals ## Sum of Squares 139343.2 144805.2 ## Deg. of Freedom 4 1699 ## ## Residual standard error: 9.231992 ## Estimated effects may be unbalanced Those coefficients do mean something, but we won’t get into that right now. More interesting for our purposes is getting the outputs of our ANOVA, which we can do by feeding anova() our model: anova(aov(lifeExp ~ continent, data = gapminder)) ## Analysis of Variance Table ## ## Response: lifeExp ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## continent 4 139343 34836 408.73 &lt; 2.2e-16 *** ## Residuals 1699 144805 85 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note, by the way, that you’d usually do this by assigning the aov() to an object (as in a &lt;- aov(...)). I’m not doing this right now, as I think this section is easier to conceptually understand if I’m explicit about what we’re running our anova() and other tests on. Looks like continent does have a significant effect on life expectancy! By the way, there’s two other, identical ways to run the same test, using lm() for aov() and summary() for anova(). It really doesn’t matter which you go with - they all return the same results. anova(lm(lifeExp ~ continent, data = gapminder)) summary(aov(lifeExp ~ continent, data = gapminder)) So now we know that there are significant differences between the continents. What we want now is to be able to tell what those differences are. Base R’s TukeyHSD() function will help with that, telling us what the differences between each continent are. All we have to do is feed it our aov() object, and, optionally, tell it to sort the results by the difference with ordered = TRUE TukeyHSD(aov(lifeExp ~ continent, data = gapminder), ordered = TRUE) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## factor levels have been ordered ## ## Fit: aov(formula = lifeExp ~ continent, data = gapminder) ## ## $continent ## diff lwr upr p adj ## Asia-Africa 11.199573 9.579887 12.819259 0.0000000 ## Americas-Africa 15.793407 14.022263 17.564550 0.0000000 ## Europe-Africa 23.038356 21.369862 24.706850 0.0000000 ## Oceania-Africa 25.460878 20.216908 30.704848 0.0000000 ## Americas-Asia 4.593833 2.664235 6.523432 0.0000000 ## Europe-Asia 11.838783 10.002952 13.674614 0.0000000 ## Oceania-Asia 14.261305 8.961718 19.560892 0.0000000 ## Europe-Americas 7.244949 5.274203 9.215696 0.0000000 ## Oceania-Americas 9.667472 4.319650 15.015293 0.0000086 ## Oceania-Europe 2.422522 -2.892185 7.737230 0.7250559 So as we can see, Africa has a significantly lower life expectancy than every other continent, followed by Asia, and then the Americas. Oceania and Europe, meanwhile, have about equal life expectancies. If you remember the tidy() function from broom that we used earlier, you probably won’t be surprised to learn it can make this sort of output tidier, too: broom::tidy(TukeyHSD(aov(lifeExp ~ continent, data = gapminder), ordered = TRUE)) ## term comparison estimate conf.low conf.high adj.p.value ## 1 continent Asia-Africa 11.199573 9.579887 12.819259 2.727152e-12 ## 2 continent Americas-Africa 15.793407 14.022263 17.564550 2.727152e-12 ## 3 continent Europe-Africa 23.038356 21.369862 24.706850 2.727152e-12 ## 4 continent Oceania-Africa 25.460878 20.216908 30.704848 2.727152e-12 ## 5 continent Americas-Asia 4.593833 2.664235 6.523432 1.047859e-09 ## 6 continent Europe-Asia 11.838783 10.002952 13.674614 2.727152e-12 ## 7 continent Oceania-Asia 14.261305 8.961718 19.560892 5.862422e-12 ## 8 continent Europe-Americas 7.244949 5.274203 9.215696 2.775336e-12 ## 9 continent Oceania-Americas 9.667472 4.319650 15.015293 8.648670e-06 ## 10 continent Oceania-Europe 2.422522 -2.892185 7.737230 7.250559e-01 This will make your life a lot better when you’re trying to export data. 4.3 Functional Programming So, it’s probably worth finding the correlation between life expectancy and GDP for each continent. We could filter our data five times - one for each continent - but that breaks our rule: if we have to do it more than twice, there’s probably a better way. Now, fair warning, what we’re about to do is probably the hardest topic we’ve gone over yet. Understanding it will make a lot of common tasks much easier, but don’t worry if you don’t understand it on the first go-through. First off, let’s load broom explicitly now - the only thing we’ll be using it for is the tidy() function. library(broom) The tidyverse includes a package - purrr - useful for working with lists. As we saw last unit, a list is a type of object which can include multiple types of data. These include the numerics, characters, and logicals we’re comfortable with - but can also include name-value pairs (like z = 0, from the last unit) and, weirdly enough, entire dataframes. For instance, let’s see what happens when we use the nest() function that purrr provides: gapminder %&gt;% nest() ## # A tibble: 1 x 1 ## data ## &lt;list&gt; ## 1 &lt;tibble [1,704 x 6]&gt; All of our data is now stored in a single cell, as a tibble! We can exclude columns from being nested with - if we want, splitting the data into as many subsets as there are levels of that variable: gapminder %&gt;% nest(-continent) ## # A tibble: 5 x 2 ## continent data ## &lt;fct&gt; &lt;list&gt; ## 1 Asia &lt;tibble [396 x 5]&gt; ## 2 Europe &lt;tibble [360 x 5]&gt; ## 3 Africa &lt;tibble [624 x 5]&gt; ## 4 Americas &lt;tibble [300 x 5]&gt; ## 5 Oceania &lt;tibble [24 x 5]&gt; And when we do this, we can subset the list in the exact same way we would a normal vector: Nested &lt;- gapminder %&gt;% nest(-continent) Nested[1, 2] ## # A tibble: 1 x 1 ## data ## &lt;list&gt; ## 1 &lt;tibble [396 x 5]&gt; If we then wanted to, we can unnest() the data: Nested &lt;- gapminder %&gt;% nest(-continent) Nested[1, 2] %&gt;% unnest() ## # A tibble: 396 x 5 ## country year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 28.8 8425333 779. ## 2 Afghanistan 1957 30.3 9240934 821. ## 3 Afghanistan 1962 32.0 10267083 853. ## 4 Afghanistan 1967 34.0 11537966 836. ## 5 Afghanistan 1972 36.1 13079460 740. ## 6 Afghanistan 1977 38.4 14880372 786. ## 7 Afghanistan 1982 39.9 12881816 978. ## 8 Afghanistan 1987 40.8 13867957 852. ## 9 Afghanistan 1992 41.7 16317921 649. ## 10 Afghanistan 1997 41.8 22227415 635. ## # ... with 386 more rows But what’s interesting is that we can also manipulate the data while it’s nested. purrr provides a number of “map” functions, which will apply a function to each member of a list and return the outputs as a list. What this allows us to do is create additional nested lists as new columns, letting us find correlations between groups much faster. For instance, if we want to create a column Cor with the output of the correlation tests, we could do the following: gapminder %&gt;% nest(-continent) %&gt;% mutate(Cor = map(data, ~ cor.test(.$lifeExp, .$gdpPercap))) ## # A tibble: 5 x 3 ## continent data Cor ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 Asia &lt;tibble [396 x 5]&gt; &lt;S3: htest&gt; ## 2 Europe &lt;tibble [360 x 5]&gt; &lt;S3: htest&gt; ## 3 Africa &lt;tibble [624 x 5]&gt; &lt;S3: htest&gt; ## 4 Americas &lt;tibble [300 x 5]&gt; &lt;S3: htest&gt; ## 5 Oceania &lt;tibble [24 x 5]&gt; &lt;S3: htest&gt; The ~ indicates that the next word is a function that should be applied to each element of the list. . is what’s referred to as a pronoun - it’s the short name for the data that’s being applied to the function. We’ll be using . repeatedly, so it’s best to internalize that meaning now. Now, because cor.test() doesn’t provide a tidy data output - it produces something human-readable, but not computer-usable - we have to tidy it up before we can extract our numbers. That’s where tidy() comes in, which we use pretty similarly to cor.test(): gapminder %&gt;% nest(-continent) %&gt;% mutate(Cor = map(data, ~ cor.test(.$lifeExp, .$gdpPercap)), TidyCor = map(Cor, ~ tidy(.))) ## # A tibble: 5 x 4 ## continent data Cor TidyCor ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 Asia &lt;tibble [396 x 5]&gt; &lt;S3: htest&gt; &lt;data.frame [1 x 8]&gt; ## 2 Europe &lt;tibble [360 x 5]&gt; &lt;S3: htest&gt; &lt;data.frame [1 x 8]&gt; ## 3 Africa &lt;tibble [624 x 5]&gt; &lt;S3: htest&gt; &lt;data.frame [1 x 8]&gt; ## 4 Americas &lt;tibble [300 x 5]&gt; &lt;S3: htest&gt; &lt;data.frame [1 x 8]&gt; ## 5 Oceania &lt;tibble [24 x 5]&gt; &lt;S3: htest&gt; &lt;data.frame [1 x 8]&gt; That last column - made up of dataframes - is exactly what we want. We can extract it from this dataframe using unnest(.drop = TRUE), which will drop the other nested columns: gapminder %&gt;% nest(-continent) %&gt;% mutate(Cor = map(data, ~ cor.test(.$lifeExp, .$gdpPercap)), TidyCor = map(Cor, ~ tidy(.))) %&gt;% unnest(TidyCor, .drop = TRUE) ## # A tibble: 5 x 9 ## continent estimate statistic p.value parameter conf.low conf.high ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia 0.382 8.21 3.29e-15 394 0.295 0.463 ## 2 Europe 0.781 23.6 4.05e-75 358 0.737 0.818 ## 3 Africa 0.426 11.7 7.60e-29 622 0.359 0.488 ## 4 Americas 0.558 11.6 5.45e-26 298 0.475 0.632 ## 5 Oceania 0.956 15.4 2.99e-13 22 0.901 0.981 ## # ... with 2 more variables: method &lt;fct&gt;, alternative &lt;fct&gt; And tada, we have the output from five correlation tests in one step, rather than the ten it would take to do the long way. When I was learning how to do this, I kept believing that it was a waste of my time, because learning took significantly more time than just doing it the long way. But in addition to the obvious benefits this method gives - we have less code replication, which means faster programs and less chance for typos; all of our data is in the same dataframe, rather than in five chunks; and we can compare our results immediately, rather than having to sift through paragraphs to find the same data points - this method is extremely applicable to many other areas of coding. If you ever have to build models or analyze grouped data, this is the way to make sure that your work is reproducible and your science is sound. So even though it might take a bit to fully internalize, keep trying these efficient paths when we use them in this reader. We’ll be returning to this format repeatedly, in order to build models, analyze results, and compare groups. 4.3.1 Column Names In order to do that, of course, we have to understand what each of those column names represent. In order: estimate is whatever parameter is being estimated - here, the correlation coefficient. statistic is the test statistic used to calculate the p value - in this case, t. p.value is, well, the p-value. It’s calculated for 95%, unless you changed the confidence level by setting cor.test(conf.level = ##). parameter is the degrees of freedom. conf.low and conf.high are the bounds of the confidence interval of the estimate - so here, of the correlation coefficient. It’s calculated for whatever confidence level you set in cor.test(conf.level = ##). method is the test used. alternative is the alternative hypothesis tested - you can change it by setting cor.test(alternative = &quot;&quot;) to “two.sided”, “lesser”, or “greater”. 4.4 Modeling Okay, time to get back to data analysis. If we want to see the effect of two continuous variables (say, gdpPercapita and year) on lifeExp, we’d use what’s known as multiple regression. There’s a lot of different flavors of multiple regression, but the simplest is what’s known as general linear regression - the y = mx + b formula that you went over in high school. R makes it easy to fit a general linear model to your data (via ordinary least squares) using the lm() function, using the same arguments as we used for aov(): lm(lifeExp ~ gdpPercap + year, data = gapminder) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap + year, data = gapminder) ## ## Coefficients: ## (Intercept) gdpPercap year ## -4.184e+02 6.697e-04 2.390e-01 These coefficients show how much we can expect life expectancy to increase if the GDP or year increases. The intercept is what life expectancy would be if both GDP and the year were 0 - obviously, this number doesn’t make a ton of sense, since we don’t have any data on life expectancy in either of those situations. If we want to see how well this model fits our data, we can graph it: ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;) Hmm. Seems like a pretty bad fit. If we want statistical estimates of how well our model fits the data, we can feed it as an argument to summary(): Model &lt;- lm(lifeExp ~ gdpPercap + year, data = gapminder) summary(Model) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap + year, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -67.262 -6.954 1.219 7.759 19.553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.184e+02 2.762e+01 -15.15 &lt;2e-16 *** ## gdpPercap 6.697e-04 2.447e-05 27.37 &lt;2e-16 *** ## year 2.390e-01 1.397e-02 17.11 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.694 on 1701 degrees of freedom ## Multiple R-squared: 0.4375, Adjusted R-squared: 0.4368 ## F-statistic: 661.4 on 2 and 1701 DF, p-value: &lt; 2.2e-16 As we can see, both GDP and the year are significant predictors of life expectancy, but the linear model isn’t a great fit for the data - our R2 is 0.44. Perhaps we could improve the model by accounting for the interaction between GDP and the year. There are two ways to include interaction terms in models, each of which have their benefits. One is to replace the + in the model with *, as follows: Model &lt;- lm(lifeExp ~ gdpPercap * year, data = gapminder) summary(Model) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap * year, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.234 -7.314 1.002 7.951 19.780 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.532e+02 3.267e+01 -10.811 &lt; 2e-16 *** ## gdpPercap -8.754e-03 2.547e-03 -3.437 0.000602 *** ## year 2.060e-01 1.653e-02 12.463 &lt; 2e-16 *** ## gdpPercap:year 4.754e-06 1.285e-06 3.700 0.000222 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.658 on 1700 degrees of freedom ## Multiple R-squared: 0.442, Adjusted R-squared: 0.441 ## F-statistic: 448.8 on 3 and 1700 DF, p-value: &lt; 2.2e-16 The other is to specify which terms you want to track the interaction of, using : as follows: Model &lt;- lm(lifeExp ~ gdpPercap + year + gdpPercap:year, data = gapminder) summary(Model) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap + year + gdpPercap:year, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.234 -7.314 1.002 7.951 19.780 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.532e+02 3.267e+01 -10.811 &lt; 2e-16 *** ## gdpPercap -8.754e-03 2.547e-03 -3.437 0.000602 *** ## year 2.060e-01 1.653e-02 12.463 &lt; 2e-16 *** ## gdpPercap:year 4.754e-06 1.285e-06 3.700 0.000222 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.658 on 1700 degrees of freedom ## Multiple R-squared: 0.442, Adjusted R-squared: 0.441 ## F-statistic: 448.8 on 3 and 1700 DF, p-value: &lt; 2.2e-16 As you can see, these outputs are identical. Even so, I personally prefer using the second format, because it makes the interaction term a more explicit part of your model. While it doesn’t matter in simple models like the one we’ve developed here, once you have a large number of terms, it’s helpful to specify which interactions you’re measuring. 4.5 Mixed Models That R2 pf 0.44 is an improvement, but still not great. We could try combining continuous and categorical variables in a single model, to try and better predict life expectancy. Doing that in R is very simple - just add the categorical variable to your model like you would a continuous. Let’s do that with continents: Model &lt;- lm(lifeExp ~ gdpPercap + year + continent + gdpPercap:year + gdpPercap:continent + year:continent + gdpPercap:year:continent, data = gapminder) summary(Model) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap + year + continent + gdpPercap:year + ## gdpPercap:continent + year:continent + gdpPercap:year:continent, ## data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.7123 -3.2453 0.1535 3.4204 17.7379 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.241e+02 3.777e+01 -11.229 &lt; 2e-16 ## gdpPercap -2.168e-02 1.226e-02 -1.768 0.07721 ## year 2.377e-01 1.909e-02 12.457 &lt; 2e-16 ## continentAmericas -4.065e+02 7.317e+01 -5.556 3.20e-08 ## continentAsia -2.258e+02 5.723e+01 -3.946 8.27e-05 ## continentEurope 7.212e+01 7.849e+01 0.919 0.35830 ## continentOceania 3.047e+02 4.846e+02 0.629 0.52956 ## gdpPercap:year 1.149e-05 6.176e-06 1.860 0.06308 ## gdpPercap:continentAmericas 7.174e-02 1.445e-02 4.963 7.63e-07 ## gdpPercap:continentAsia 7.051e-03 1.251e-02 0.563 0.57318 ## gdpPercap:continentEurope 4.049e-02 1.317e-02 3.075 0.00214 ## gdpPercap:continentOceania 1.275e-02 3.527e-02 0.362 0.71768 ## year:continentAmericas 2.119e-01 3.694e-02 5.736 1.15e-08 ## year:continentAsia 1.196e-01 2.894e-02 4.133 3.75e-05 ## year:continentEurope -2.636e-02 3.970e-02 -0.664 0.50684 ## year:continentOceania -1.408e-01 2.461e-01 -0.572 0.56739 ## gdpPercap:year:continentAmericas -3.634e-05 7.275e-06 -4.996 6.46e-07 ## gdpPercap:year:continentAsia -3.945e-06 6.305e-06 -0.626 0.53163 ## gdpPercap:year:continentEurope -2.076e-05 6.629e-06 -3.133 0.00176 ## gdpPercap:year:continentOceania -6.939e-06 1.757e-05 -0.395 0.69289 ## ## (Intercept) *** ## gdpPercap . ## year *** ## continentAmericas *** ## continentAsia *** ## continentEurope ## continentOceania ## gdpPercap:year . ## gdpPercap:continentAmericas *** ## gdpPercap:continentAsia ## gdpPercap:continentEurope ** ## gdpPercap:continentOceania ## year:continentAmericas *** ## year:continentAsia *** ## year:continentEurope ## year:continentOceania ## gdpPercap:year:continentAmericas *** ## gdpPercap:year:continentAsia ## gdpPercap:year:continentEurope ** ## gdpPercap:year:continentOceania ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.307 on 1684 degrees of freedom ## Multiple R-squared: 0.7643, Adjusted R-squared: 0.7616 ## F-statistic: 287.4 on 19 and 1684 DF, p-value: &lt; 2.2e-16 Our R2 is now up to a respectable 0.76! It looks like these three variables - and the interactions between them - explain a lot of the variance in our data. To get a cleaner table explaining the impacts each variable has on life expectancy, we can perform an analysis of covariance, or ANCOVA. To do this, we just change out our mixed model lm() function for an aov(), and then pass that aov() object to summary(): Model &lt;- aov(lifeExp ~ gdpPercap + year + continent + gdpPercap:year + gdpPercap:continent + year:continent + gdpPercap:year:continent, data = gapminder) summary(Model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gdpPercap 1 96813 96813 2434.192 &lt;2e-16 *** ## year 1 27495 27495 691.316 &lt;2e-16 *** ## continent 4 79428 19857 499.269 &lt;2e-16 *** ## gdpPercap:year 1 18 18 0.461 0.497 ## gdpPercap:continent 4 5420 1355 34.069 &lt;2e-16 *** ## year:continent 4 4331 1083 27.225 &lt;2e-16 *** ## gdpPercap:year:continent 4 3667 917 23.048 &lt;2e-16 *** ## Residuals 1684 66976 40 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This output means we can now say that GDP has a significant impact on life expectancy when controlling for year as a covariate - and that all the interaction terms are significant, except for GDP and year. One way to get around having to use an ANCOVA is to develop separate regression models for each continent. We can do that in much the same way we tested for correlation on each continent: gapminder %&gt;% nest(-continent) %&gt;% mutate(Models = map(data, ~ lm(lifeExp ~ gdpPercap + year + gdpPercap:year, data = .)), TidyMods = map(Models, ~ tidy(.))) %&gt;% unnest(TidyMods, .drop = TRUE) ## # A tibble: 20 x 6 ## continent term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia (Intercept) -650. 54.6 -11.9 4.38e-28 ## 2 Asia gdpPercap -0.0146 0.00318 -4.60 5.71e- 6 ## 3 Asia year 0.357 0.0276 12.9 4.34e-32 ## 4 Asia gdpPercap:year 0.00000754 0.00000161 4.68 3.97e- 6 ## 5 Europe (Intercept) -352. 29.8 -11.8 1.92e-27 ## 6 Europe gdpPercap 0.0188 0.00208 9.06 8.85e-18 ## 7 Europe year 0.211 0.0151 14.0 6.51e-36 ## 8 Europe gdpPercap:year -0.00000928 0.00000104 -8.90 2.80e-17 ## 9 Africa (Intercept) -424. 41.9 -10.1 2.09e-22 ## 10 Africa gdpPercap -0.0217 0.0136 -1.59 1.11e- 1 ## 11 Africa year 0.238 0.0212 11.2 8.83e-27 ## 12 Africa gdpPercap:year 0.0000115 0.00000685 1.68 9.40e- 2 ## 13 Americas (Intercept) -831. 54.3 -15.3 2.46e-39 ## 14 Americas gdpPercap 0.0501 0.00663 7.55 5.53e-13 ## 15 Americas year 0.450 0.0274 16.4 1.81e-43 ## 16 Americas gdpPercap:year -0.0000249 0.00000333 -7.46 9.52e-13 ## 17 Oceania (Intercept) -119. 41.3 -2.89 9.07e- 3 ## 18 Oceania gdpPercap -0.00892 0.00283 -3.15 5.01e- 3 ## 19 Oceania year 0.0969 0.0210 4.62 1.67e- 4 ## 20 Oceania gdpPercap:year 0.00000455 0.00000141 3.23 4.19e- 3 Note that I deleted all the “continent” terms from the model, because we’re now making a different model for each continent available. I’ve also used the . pronoun to represent our data here. In this case, estimate represents the coefficient (beta) for each variable, while the statistic is the F statistic. 4.6 Conclusion The important takeaways from this unit are not necessarily the statistical tests used - since those will vary dependent upon your purpose - so much as the methods highlighted. Understanding how to generate hypotheses from a new dataset - and then how to drill down and analyze them each in turn - is a cross-disciplinary skill used in any new analysis project. Later in the course, you’ll be given projects which will require you to understand brand new datasets, manipulate them, perform complex analyses on them, and visualise them. EDA will give you the familiarity with your data to find these patterns, isolate them appropriately, and perform the right analyses moving forward. The entire purpose of this unit is to give you the skillset to identify what those analyses might be, by understanding how to generate hypotheses from a combination of data visualization and manipulation. What we haven’t done is confirmatory analysis, where we start our work with a model in mind and then report how well it fit the data. Once data is used for exploratory analysis, it can’t be used again for confirmatory work. There are methods to split data for confirmatory analysis, while still being able to use some data for exploration - I like Hadley Wickham’s overview here. We won’t go too far into the difference between the methods, as this isn’t a statistics course. For our purposes in this course, all of our analyses will be exploratory. 4.7 Exercises 4.7.1 Answer the following: The output of psych::describe(gapminder_unfiltered) put stars after country and continent. Why? Make a histogram of a gapminder variable other than population. Describe the graph with regards to its skewdness and kurtosis. Compute an ANOVA for the impacts of continent on population. Report the results as you would in a manuscript. Fit a regression model to the impacts of the current year and life expectancy on GDP. Why does this model not make sense conceptually? Fit a linear model explaining life expectancy as a function of the current year for each country in the dataset (use the formula lifeExp ~ year). Then tidy the model outputs and look at the p values for each coefficient. If you don’t hate yourself, you’ll try the functional computing approach. "],
["functions-and-scripting.html", "5 Functions and Scripting 5.1 Writing Functions 5.2 Conditional Statements 5.3 Saving and Loading Functions 5.4 Loops 5.5 Exercises", " 5 Functions and Scripting 5.1 Writing Functions By now, we’ve started understanding how to use the prebuilt functions available for us within R. However, those functions don’t cover everything that we might need to do - and so sometimes, we need to build them for ourselves. At first, it might not make sense to spend your time writing functions, when you could just copy and paste the same code snippets to multiple places. But - in addition to this failing our rule that, if you have to repeat something more than twice, there’s a better way - copying and pasting increases the chance of typos, makes the important parts of your code harder to understand, and makes your script or notebook unnecessarily long. Additionally, if the needs of your project change - or you catch a mistake in your code - functions only need to be changed in one place. Plus, once we move into working with others and sharing your code, functions make cleaning data and standardizing analyses between partners much easier. 5.1.1 Our First Function Some functions might seem impossibly complex - coding something like ggplot’s geoms, for instance, is probably a bit beyond us right now. But others really aren’t that hard - for instance, if we wanted to code another function to calculate the mean, our code would look like this: mean2 &lt;- function(x){ MEAN &lt;- sum(x)/length(x) return(MEAN) } Note if we want to run an entire chunk of code while using curly braces ({}), we have to run our code from the very first line - trying to run the code from the middle will only evaluate the section inside of those particular braces. As you can see, there are three steps to this process: We defined the object mean2() as a function (using function()), which takes the argument x Inside the curly braces {}, we coded what the function should do - define MEAN as the sum of x divided by the number of elements in x We told R what our function should return() as an output - in this case, MEAN This is how most functions work! While the internal code can be a lot more complicated than this, at their essence, functions are just objects that manipulate their inputs before returning an output. We can compare the results of our function against base R’s mean(), to make sure we did alright: mean(iris$Sepal.Length) ## [1] 5.843333 mean2(iris$Sepal.Length) ## [1] 5.843333 Not bad! 5.1.2 Returns As a quick sidenote, not all the code you’ll see in the wild will be quite as explicit about what it return()s. For instance, the same code will run exactly like mean2() did, and is a bit shorter: mean3 &lt;- function(x){ sum(x)/length(x) } In general, functions will return the last thing they evaluate. return() is typically used to stop a function early - if it fails a conditional (as we’ll discuss below) or other similar cases. I personally like being explicit about what a function returns, but most developers will let their function return values implicitly instead. 5.1.3 More Complicated Functions Where other functions get more complicated is in the number of arguments they take. In mean2(), we defined x as our only argument. This is the data argument - it tells us what dataset we should use in our function. However, plenty of functions have detail arguments as well - for instance, the na.rm argument in mean(), or the method argument in cor.test(). These allow us to specify exactly how our function is implemented. For instance, if we wanted to code a new function, multistat(), which let us run any function on a dataset x: multistat &lt;- function(x, func = mean){ func(x) } We could now use that function to calculate standard deviation, for instance: multistat(iris$Sepal.Length, sd) ## [1] 0.8280661 But if we left the func argument blank, it would default to what it was defined as - in this case, mean: multistat(iris$Sepal.Length) ## [1] 5.843333 This is a pretty stupid reason to make a function - it doesn’t do anything that the base functions don’t, and makes your code harder to understand - but it’s good for demonstration purposes. You don’t have to give your detail arguments a default value - and it’s often helpful if you don’t, as it makes you be explicit about what you want as an output. However, if you’re expecting that you - or anyone else using your function - will usually want a particular output, you can specify the default using = like we did above. 5.1.4 About Names… You might remember our discussion in unit 1 about why R occasionally requires quotes around things, while othertimes it doesn’t. This is what our explanation was then: Note the quotes around “tidyverse” when you go to install it, but not when it’s inside of library(). The reason for this is a little complicated - basically, you don’t use quotes for things that are inside of R’s memory, like data, functions, and packages. You use quotes for everything else. This is still true inside of functions, but with a twist - objects that are defined inside of functions are only defined inside of that function. For instance, even though our mean functions have assigned Sepal.Length to x, we can’t then call mean(x). As such, there’s not as much worry about creatively naming objects created inside of functions - while your names should still be short and descriptive, they don’t need to be distinct from names that exist outside of the function. 5.2 Conditional Statements Let’s go back to our mean() copycat functions. Now, to be fully honest, the code for mean() is a little more complicated - the function checks to make sure your vector isn’t a character vector, removes NA values if specified, trims your data, and then calculates the mean. But we’ve come close with our basic functions! In order to come a bit closer, we’d have to make use of something known as a conditional statement. In their simplest form, these statements evaluate whether or not something is true, then return the appropriate output. The most basic version of a conditional is the if() statement. if() evaluates the statement inside of its parentheses, and then returns the result of whatever code is in the {} below it: if(TRUE == 1){ &quot;Yes&quot; } ## [1] &quot;Yes&quot; Typically, you’ll see if() statements paired with else statements, which will run the code below them if the conditional statement is false. else statements should be surrounded by {} brackets for clarity. For instance: if(TRUE == 0){ &quot;Yes&quot; } else { &quot;No&quot; } ## [1] &quot;No&quot; You can even combine the two: if(TRUE == 0){ &quot;Yes&quot; } else if(FALSE == 1){ &quot;Maybe&quot; } else { &quot;No&quot; } ## [1] &quot;No&quot; A shorter version of this format is the ifelse() statement, which works much like if statements in Excel - it evaluates the conditional statement, then returns the first value if the statement is true, or the second if it’s false: ifelse(TRUE == 1, 4, 0) ## [1] 4 If we wanted to get closer to the mean() function, then, we could do something like this: mean4 &lt;- function(x){ if(!is.numeric(x)) { return(&quot;That&#39;s not a number!&quot;) } else{ MEAN &lt;- sum(x)/length(x) return(MEAN) } } This function does exactly as well handling numeric values as our other functions: mean4(iris$Sepal.Length) ## [1] 5.843333 But makes a bit more sense when handed other types of values: mean2(&quot;h&quot;) ## Error in sum(x) : invalid &#39;type&#39; (character) of argument mean4(&quot;h&quot;) ## [1] &quot;That&#39;s not a number!&quot; 5.2.1 Stops The proper way to handle that sort of error-catching is to use stop() statements. While our mean4() function will ID when it’s given a non-numeric dataset, it will still return a value - in this case, the string “That’s not a number!” The problem with this is that it won’t make it obvious that something’s gone wrong - it lets the mess-up be implicit, instead of explicit. We can fix that by giving the error message to stop() instead of return: mean5 &lt;- function(x){ if(!is.numeric(x)) { stop(&quot;That&#39;s not a number!&quot;) } else{ MEAN &lt;- sum(x)/length(x) return(MEAN) } } mean5(&quot;h&quot;) ## Error in mean5(&quot;h&quot;) : That&#39;s not a number! If we think that this sort of error is worth alerting the user about, but not stopping the entire function, we can use warning() to generate warning messages: mean6 &lt;- function(x){ if(!is.numeric(x)) { warning(&quot;That&#39;s not a number! Returning NA.&quot;) return(NA_real_) } else{ MEAN &lt;- sum(x)/length(x) return(MEAN) } } mean6(&quot;h&quot;) ## Warning in mean6(&quot;h&quot;): That&#39;s not a number! Returning NA. ## [1] NA You might have noticed, by the way, that I haven’t been using descriptive object names for our mean() functions. That’s because it’s hard to come up with short, descriptive names for such similar objects - this is the problem that we solve with %&gt;% for our datasets! But still, I tripped up once or twice while writing this chapter, and accidentally called the wrong function - you should make sure you’re naming your functions much more descriptively than I am! 5.2.2 Function Dependencies As you’ve seen, we can include functions inside of our functions - and, in fact, most of the most useful functions do exactly this. In that last example, we used mean() and sd() inside of our multistat() function - both of which are included in base R. However, we can even use functions from other libraries if we want. For instance, we can use the describe() function from psych below - but note that we’re preceeding it with psych::, so that it’ll run even if the user doesn’t have the psych library loaded: summary_describe &lt;- function(x){ return(list(Summary = summary(x), Describe = psych::describe(x))) } summary_describe(iris$Sepal.Length) ## $Summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 The problem with doing this is that if your end user doesn’t have the package installed, your function will fail out. If you want to force your users to download that package, we’d code something like this: summary_describe &lt;- function(x){ if(!require(psych)){ install.packages(&quot;psych&quot;) return(list(Summary = summary(x), Describe = psych::describe(x))) } else{ return(list(Summary = summary(x), Describe = psych::describe(x))) } } summary_describe(iris$Sepal.Length) ## Loading required package: psych ## $Summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 (We use require() in the place of library() here as it will generate a warning message - letting our function proceed - rather than an error. More info here.) We can also require that packages be loaded in an R session, using code like this: summary_describe &lt;- function(x){ if(!require(psych)){ install.packages(&quot;psych&quot;) library(psych) return(list(Summary = summary(x), Describe = describe(x))) } else{ return(list(Summary = summary(x), Describe = describe(x))) } } summary_describe(iris$Sepal.Length) ## $Summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 This isn’t a great practice, though - you should only load a library inside a function if you use that library often enough that using :: becomes impractical. Otherwise, your code becomes hard for outsiders to understand - you might understand where describe() comes from, but if you have a ton of packages loaded for a function, other users will have to go digging. Also, make sure to be careful with how many other packages your functions depend upon - the more packages, the more chances something breaks when one of them updates! 5.2.2.1 Sidenote: Note that I had to use “list” in summary_describe() above to return more than one object - and that both objects are written with a $ in front of their name. One cool side effect of this is that I can ask R to return only one of the outputs: summary_describe(iris$Sepal.Length)$Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 This has some actual applications when using tidyverse functions - for instance, summarise() can only use single-output functions. We can get around this by doing the following: library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.0.0 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() -- ## x ggplot2::%+%() masks psych::%+%() ## x ggplot2::alpha() masks psych::alpha() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() iris %&gt;% group_by(Species) %&gt;% summarise(Estimate = cor.test(Sepal.Length, Sepal.Width)$estimate, pValue = cor.test(Sepal.Length, Sepal.Width)$p.value) ## # A tibble: 3 x 3 ## Species Estimate pValue ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 0.743 6.71e-10 ## 2 versicolor 0.526 8.77e- 5 ## 3 virginica 0.457 8.43e- 4 This is another workaround to the map functions we described in unit 4. Map functions have a lot of advantages to this approach - for instance, they don’t require repeating yourself as much, and they run faster - but there’s nothing wrong with doing it this way. However, not all functions return lists, so you may have some challenges if you only rely on this approach. 5.3 Saving and Loading Functions If you want to use a function in multiple scripts or notebooks, you have to save it in its own .r file - that is, its own R script file. After doing so, you’ll be able to load the function in other scripts and notebooks using the source() command, with the filename (in quotes, with the extension, case sensitive) as the only argument. Once you do that, you’ll be able to use the function as normal. Saving and loading functions in this way has a lot of the same benefits as writing functions in the first place - it cuts down on repetition, makes editing and debugging easier, and makes your code easier for other people to understand. 5.4 Loops You may have noticed a theme throughout this reader - repetition is bad. Repeating your code makes it easy to make mistakes, and makes it harder to edit things as needed. As such, there’s a concept in R - and most other programming languages - called looping, designed to cut down on repetitions. For instance, say we had a tibble: df1 &lt;- tibble(a = c(1,1,1,1), b = c(2,2,2,2)) If we wanted to multiply each column by 2, we could do the following: df1$a &lt;- df1$a * 2 df1$b &lt;- df1$b * 2 df1 ## # A tibble: 4 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2. 4. ## 2 2. 4. ## 3 2. 4. ## 4 2. 4. But as usual, we want to find a way to reduce that repetition. The best tool in R for this sort of thing is what’s known as a for loop, which will repeat an action a specified number of times. To complete this task, we’d write a for loop that looked something like this: df1 &lt;- data.frame(a = c(1,1,1,1), b = c(2,2,2,2)) for (i in seq_along(df1)){ df1[[i]] &lt;- df1[[i]] * 2 } df1 ## a b ## 1 2 4 ## 2 2 4 ## 3 2 4 ## 4 2 4 Alright, so let’s break that down. The for() statement initializes the loop, telling R that we’re going to want to do something repeatedly. Inside the parentheses, the i in seq_along(df1) defines how many times we want to loop the code. This will make a little more sense if we see what we get from running seq_along(df1) by itself: seq_along(df1) ## [1] 1 2 seq_along() returns the position of each column of a dataframe or each element in a vector - so for a dataframe with n columns, we’ll get the output “1, 2, …, n”. While this isn’t particularly useful by itself, it becomes powerful when used to make loops. The code i in seq_along(df1) then tells our code to repeat itself as many times as there are positions. It does this by incrementing i by 1 each time the code is run, with the first iteration having a value of 1. I should note that it doesn’t matter what you use to represent i - while i is extremely common, you could use almost any object name. We then are able to act on each column of our dataframe by selecting it using [[i]]. Since i increases by 1 each time the code is looped, we keep selecting the next column in order, until we’re completely done with our loop. If we want to make a new dataframe with our output, we have to be careful to initalize it with the proper number of columns before we start our loop - if we don’t, our code will slow down significantly. Say we wanted our function to return the median of each column. Doing this the right way looks something like this: df1 &lt;- data.frame(a = c(1,1,1,1), b = c(2,2,2,2)) out &lt;- vector(&quot;numeric&quot;, length(df1)) out[1] &lt;- df1[1] for (i in seq_along(df1)){ out[[i]] &lt;- median(df1[[i]]) } out ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 By the way - you may have heard or read that for loops are extremely slow in R. That’s not true, anymore. While they’re slower than Python’s loops by a good margin, that’s because everything is slower than Python by a good margin - but R is pretty middle-of-the-road when it comes to loop speed. That being said, most programmers in R avoid using for loops when possible. That’s both because loops were significantly slower in R than other languages not that long ago, but also because the nature of R makes using functions more popular and efficient than loops. But there are certain times loops are unavoidable - and we’ll be using them in the next unit! For more on for loops, look at Hadley Wickham’s coverage of the subject. For our purposes, we don’t have to go much further than we already have on the subject. 5.5 Exercises Remember in unit 3 that there’s no standard error function in base R. Write one, naming it std.err. Write a function that will say “Hi!” if given the input “hi”, “Bye!” if it gets the input “bye”, and “How rude!” if the input isn’t either of those. Use a loop to find the mean and median of each column of this dataframe: df &lt;- data.frame(x = c(57, 24, 245, 3526), y = c(67, 234, 574, 57)) "],
["more-complicated-analyses.html", "6 More Complicated Analyses 6.1 Work in Progress 6.2 Other Datasets", " 6 More Complicated Analyses 6.1 Work in Progress 6.2 Other Datasets 6.2.1 Importing Your Own Data So far in this course, we’ve been working exclusively with the data pre-available in R and a few of the packages we’ve taken advantage of. While this is a good way to learn R, it’s not particularly helpful in letting you complete your own analyses. Luckily, R has a number of functions used to import data from external files. To demonstrate these, I’ll be using datasets located at https://github.com/mikemahoney218/Unit5Data, in the “Datasets” folder. You don’t necessarily need to download these files - but we’ll be using these for demonstration throughout this unit. We’re going to be assuming that your data are located in the same folder as your script - this is usually the easiest way to manage datasets. Otherwise, you’ll have to wrestle a little with using relative pathways and directory names - Hadley Wickham explains these in more detail here. In order to do this, we’ll be using the readr package, which is included in the base tidyverse: library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.0.0 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() All the datasets included in our example use different delimiters - the character that tells R (or Excel, or whatever program you’re using) where one column stops and the next one begins. No matter what your delimiter is, we can parse the file using read_delim() - the first argument is the filename to read, while the second is the delimiter used. For instance, our text file is tab delimited - and since tabs are represented in R as \\t, we’ll tell read_delim() to use that as a delimiter: SiteData &lt;- read_delim(&quot;SiteData.txt&quot;, &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## Site = col_character(), ## LakeRiver = col_character(), ## Elevation = col_integer(), ## Lat = col_double(), ## Long = col_double() ## ) SiteData ## # A tibble: 19 x 5 ## Site LakeRiver Elevation Lat Long ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alder Pond Lake 327 43.9 -73.7 ## 2 Bear Creek River 477 43.6 -75.1 ## 3 Cedar Creek River 470 43.8 -74.3 ## 4 Dog Pond Lake 583 44.1 -74.7 ## 5 Duane Stream River 395 44.7 -74.2 ## 6 Fish Creek River 468 44.3 -74.2 ## 7 Gull Pond Lake 350 43.8 -73.7 ## 8 Indian River River 660 43.5 -74.6 ## 9 Twin Lake Lake 592 43.5 -74.9 ## 10 Lyon&#39;s Brook River 474 44.4 -74.1 ## 11 Masawepie Creek River 490 44.0 -75.1 ## 12 Mill Creek River 520 43.4 -74.9 ## 13 Pond Two Lake 664 44.2 -74.7 ## 14 Moose Creek River 695 44.2 -74.1 ## 15 Death Brook River 611 43.8 -74.7 ## 16 Rock Pond Lake 485 44.2 -74.3 ## 17 Copperas Pond Lake 538 44.3 -73.9 ## 18 Whitney Creek River 729 43.6 -74.6 ## 19 Towers Brook River 971 43.5 -74.2 We can do the same thing with documents that have comma separated values (known as CSVs): CoverData &lt;- read_delim(&quot;CoverData.csv&quot;, &quot;,&quot;) ## Parsed with column specification: ## cols( ## Site = col_character(), ## Transect = col_integer(), ## Plot = col_character(), ## Quadrant = col_integer(), ## CoverType = col_character() ## ) CoverData ## # A tibble: 756 x 5 ## Site Transect Plot Quadrant CoverType ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Bear Creek 1 A 1 Litter ## 2 Bear Creek 1 A 2 Grass ## 3 Bear Creek 1 A 3 Litter ## 4 Bear Creek 1 A 4 Litter ## 5 Bear Creek 1 B 1 Litter ## 6 Bear Creek 1 B 2 Grass ## 7 Bear Creek 1 B 3 Grass ## 8 Bear Creek 1 B 4 Regen ## 9 Bear Creek 1 C 1 Regen ## 10 Bear Creek 1 C 2 Moss ## # ... with 746 more rows However, readr also includes a pretty good tool specifically for CSV files: CoverData &lt;- read_csv(&quot;CoverData.csv&quot;) ## Parsed with column specification: ## cols( ## Site = col_character(), ## Transect = col_integer(), ## Plot = col_character(), ## Quadrant = col_integer(), ## CoverType = col_character() ## ) CoverData ## # A tibble: 756 x 5 ## Site Transect Plot Quadrant CoverType ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Bear Creek 1 A 1 Litter ## 2 Bear Creek 1 A 2 Grass ## 3 Bear Creek 1 A 3 Litter ## 4 Bear Creek 1 A 4 Litter ## 5 Bear Creek 1 B 1 Litter ## 6 Bear Creek 1 B 2 Grass ## 7 Bear Creek 1 B 3 Grass ## 8 Bear Creek 1 B 4 Regen ## 9 Bear Creek 1 C 1 Regen ## 10 Bear Creek 1 C 2 Moss ## # ... with 746 more rows Now, readr doesn’t have native support for more complicated files, like Excel files. Philosphically, you shouldn’t store data in Excel format for long periods of time - we have no idea how long Microsoft will be around for, and the encoding used by Excel may someday disappear and take your data with it. CSVs are generally preferred for long-term data storage, as they’re easy to understand visually and are easily parsed by computers. However, data entry is much easier in Excel, and plenty of data professionals still use the format. Luckily, there’s a package - readxl - designed to parse these types of files: install.packages(&quot;readxl&quot;) library(readxl) PlotData &lt;- read_excel(&quot;PlotData.xlsx&quot;) PlotData ## # A tibble: 189 x 5 ## Site Transect Plot Slope LightDots ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bear Creek 1. A 2 96 ## 2 Bear Creek 1. B 3 96 ## 3 Bear Creek 1. C 4 8 ## 4 Bear Creek 2. A 2 96 ## 5 Bear Creek 2. B 2 96 ## 6 Bear Creek 2. C 15 9 ## 7 Bear Creek 3. A 2 96 ## 8 Bear Creek 3. B 2 96 ## 9 Bear Creek 4. A 2 96 ## 10 Bear Creek 4. B 2 96 ## # ... with 179 more rows You’ll also notice that RStudio has an “import dataset” button in the top right corner, which makes use of both the readr and readxl packages. This button lets you point and click your way through data import, and then copy the code into your script. It’s a great resource for beginners! 6.2.2 Exporting Data Writing data to a file is also pretty painless using readr. There are as many options for delimiters as before - you can use write_delim() to specify which you want to use with your data - but more commonly data is imported and exported as CSV files using write_csv() write_csv(CoverData, &quot;CoverData.csv&quot;) Note that, like all other tidyverse functions, the dataset is the first thing you specify. INCLUDE JOINS AND AIC "],
["achieving-graphical-excellence.html", "7 Achieving Graphical Excellence 7.1 Work in Progress", " 7 Achieving Graphical Excellence 7.1 Work in Progress library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.0.0 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(cowplot) ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave https://www.jstor.org/stable/2288400?seq=1#metadata_info_tab_contents http://www.psych.utoronto.ca/users/spence/Spence%202005.pdf http://vis.stanford.edu/files/2010-MTurk-CHI.pdf http://www.ggplot2-exts.org/ Look at this scatterplot: Hopefully we can already tell that this isn’t a great graph. The complete lack of text means we have no idea what data are being visualized, or what the takeaway message is supposed to be. Remember, graphs are for storytelling and demonstrating your point, not necessarily for giving exact values - you should include tables in your document if the exact values are important. Even so, we can tell just from this scatterplot which points have larger values than others - they’re the ones further up and to the right. That’s because we’ve been trained to see position as an ordered aesthetic in graphs. Position isn’t the only way to communicate which values are larger than others. For instance, if we want to show the level of a third variable, we can use color: Which of these have a larger value of that third variable? Most people would assume the darker colors have the larger values, due to their higher contrast with the background. If we make the contrast less obvious, it becomes much harder to tell what the color is supposed to convey: But at the same time, even contrast isn’t quite enough for us to automatically interpret a color in a graph. For instance, the rainbow colors have different amounts of contrast against a white background, but when plotted: Which values are higher now? Humans make connections between different colors based on their luminance and chroma. We won’t go wading too far into what those terms mean; but it’s important to understand that the actual colors you use to compare values matter less than the differences in their shade and intensity. Moving away from color, we can also use other aesthetics to communicate a third variable. For instance: Which values are larger? We have one last aesthetic that we can use to show our third variable - the shape of the points: Which values are larger? As we can see, some aesthetics communicate quantitative data very well, while others should only be used for qualitative purposes. We already knew this - we touched on it in our first unit. But getting a sense of what representations are appropriate for our data - and what sorts of things we’re able to do with it - is the first step towards creating worthwhile graphics for whatever business or research purpose you have. Different Animation paper on silencing github commenting your code "]
]
